[["index.html", "Introduction to R for Econometrics Chapter 1 Overview 1.1 Motivation 1.2 Contents 1.3 Why R? 1.4 Downloading R and RStudio 1.5 Further reading", " Introduction to R for Econometrics Kieran Marray `Econometrics 1 - Tinbergen Institute Chapter 1 Overview 1.1 Motivation This is a short introduction to R to go with the first year econometrics courses at the Tinbergen Institute. It is aimed at people who are relatively new to R, or programming in general. The goal is to give you enough of knowledge of the fundamentals of R to write and adapt code to fit econometric models to data, and to simulate your own data, working alone or with others. You will be able to easily read in data into a dataframe, plot it, manipulate it into the form you want, fit models based on routines others have built, and present the results in a nice format. Most importantly, when things inevitably go wrong, you will be able to begin to interpret error messages and adapt others solutions to fit your needs. Of course, this will not be easy to begin with. There is no substitute for experience. But hopefully this introduction will give you a good start. This is a work in progress, and certainly not a definitive guide to R. If you see something that is wrong or that would be useful to add, please let me know. 1.2 Contents In the first section, we introduce the basics - objects and functions you need to know to work effectively in R. In the second, we go through some basic data analysis and cleaning: how to store data, plot it, and work with it. In the third, we go through how to implement models using packages - sets of functions that other people in the community have developed. We do this using the linear regression object, as it is the core of many other econometric models. At the end of this section, we list packages that others have written that may help you in your assignments. In the fourth, we show how to take the results of our estimators and automatically generate nice reports. In the fifth section, we cover some more advanced material that might be useful if you would like to do more complicated things with R like programming your own estimators. 1.3 Why R? R is a free, open-source programming language specifically designed for statistical programming. It is a great language to use for econometrics, data science, and statistics, as it combines the best parts of both pure programming languages like Julia with the best parts of pre-built statistical software like Stata. R centers around packages - sets of functions built by others in the community that you can load and use yourself. This means that, unlike say Julia, you can easily find a routine with good documentation that someone else has built to fit whatever econometric model you want without having to program it yourself. But R is an actual programming language unlike Stata or SPSS. Thus, you can write your own routines or perform basic data analysis without it causing you a massive headache. R is very widely used in academia and industry, for example in companies like Google and Uber. This means that there is a large and vibrant community of R programmers online who are keen to help others with their problems. They answer questions, organise conferences, and even make podcasts where they talk about data science and/or coffee https://nssdeviations.com/ . 1.4 Downloading R and RStudio To use R, you first need to download it. I recommend downloading the latest release from here https://cran.r-project.org/src/base/R-4/. You should also use an IDE (interactive development environment) to program in R initially. I recommend downloading RStudio https://www.rstudio.com/products/rstudio/ . 1.5 Further reading There are many great R resources online. In particular, the Chief Scientist at RStudio, Hadley Wickham, makes lots of good guides to R available for free on his website http://hadley.nz/.I recommend checking out R for Data Science. Once you have done some programming in R, Advanced R is a really thorough overview of all the intricacies. RStudio publish some great R cheatsheets https://www.rstudio.com/resources/cheatsheets/. For econometrics, Nick Huntingdon-Klein has a great collection of resources on his website https://nickchk.com/econometrics.html. "],["foundations.html", "Chapter 2 Foundations 2.1 Objects 2.2 Vectors 2.3 Functions 2.4 Iteration 2.5 Style", " Chapter 2 Foundations Here we go over the basic commands in R, and how R is best used. R is a vectorised programming language - it is fastest when you perform operations on vectors. Consequently, it is better to put your data in vectors, and perform operations on vectors, as opposed to using normal programming tricks like loops. We will explain this in more detail later. Details about the language might seem irrelevant if you just need to work with some data and fit some regression models. They are not. Knowing something about how the language works is for understanding how you fit the model and what the output is like. Thus, it is better to cover these basics before starting to work with any data. 2.1 Objects The first thing you need to know is how to create objects with names. This is how you store things in memory to use them later. In R, this is done using a a left arrow. We are going to create a string - a sequence of symbols between double quotes - and store it with a name. rm(list = ls()) # Ignore the thing above for now - I will explain what that is doing later first_string &lt;- &quot;hello_world!&quot; We did two things in this line of code. Firstly we created an object, the string hello_world!. Then, we bound it to the name first_string. The first character in the name can be anything that does not start with a number. string_3 is admissible, but 3_string is not. Naming an object stores it in memory, and allows us to access it later on in the code. So now if we want to access that string, we can type first_string ## [1] &quot;hello_world!&quot; and R gives us the string. We can manually take a look at the objects we have stored in memory. Open RStudio and take a look at the top right of your screen. There should be a pane called environment. This is a record of every single object you have stored in your working memory. You should now see an object called first_string. If you click on it, you can see what the object is. Sometimes, we will want to remove an object from our working memory. We can do this using rm, which removes an object from your environment. If, for example, we wanted to remove first_string we would type rm(first_string) Note that R does not do this automatically when we run a new script. Thus, at the beginning of each script, we should clear the environment. We do this by running rm(list=ls()) as the very first line of the script. This is what we did at the very beginning of this section. Otherwise, it is very easy to introduce errors by accidentally using variables defined in another script in a current script. 2.2 Vectors Vectors are the most important type of object in R. Fundamentally, a vector is an ordered set of values like strings or numbers. Vectors will be the basis of the objects we use to store data later. We create them by putting elements in between c(), separated by commas. first_vec &lt;- c(0,1,2,3,4) The elements of a vector are ordered. R stores them in the order in which we pass them. Thus, 0 is the first element of our vector, 1 is the second and so on. There are two types of vector: atomic vectors, and lists. The difference is that in an atomic vector, all the elements must be of the same type. In a list, the elements can be of different types. For ease, we will start by going over the basics of atomic vectors. Then, we will move on to lists. 2.2.1 Atomic vectors There are four common types of atomic vector you will come across (plus two other rare types, that we can safely ignore for now). These are logicals, doubles, integers, and characters. Logicals are vectors of logical operators - things that are either TRUE, or FALSE. We can create logicals based on conditions using the == sign. == returns TRUE if the objects on both sides are exactly equal to each other, and FALSE otherwise. # creating some logical conditions 3==4 ## [1] FALSE &quot;cat&quot;==&quot;dog&quot; ## [1] FALSE f &lt;- &quot;cat&quot; &quot;cat&quot;== f ## [1] TRUE This will come in very handy when we want to select some variables based on the values of other variables, as we commonly do. We can also write TRUE, FALSE more succinctly as T,F. # Two equivalent ways of writing logicals (1+1==2)==TRUE ## [1] TRUE (1+1==2)==T ## [1] TRUE (1+1==3)==FALSE ## [1] TRUE (1+1==3)==F ## [1] TRUE Integer is what is sounds like - a vector of integers. We use doubles to represent numbers that may not be integers. A double can be any type of number, including integers and decimals (or floating point numbers as programmers often call them). We can generate a vector containing a range of integers by passing a starting integer, followed by a colon, followed by the final integer. Finally, character vectors are vectors of strings. # Lets make a vector of each type first_log_vec &lt;- c(TRUE, FALSE, TRUE) first_int_vec &lt;- c(1,2,3) int_range_vec &lt;- c(1:10) first_double_vec &lt;- c(1.1, 2.2, 3.3) first_char_vec &lt;- c(&quot;my&quot;, &quot;first&quot;, &quot;character&quot;, &quot;vector&quot;) typeof() tells you the type of your vector, and length() tells you how many elements are in it. # Looking the type of the first vector, and length of the second typeof(first_log_vec) ## [1] &quot;logical&quot; length(first_int_vec) ## [1] 3 You can merge two atomic vectors by putting them in a larger vector. # creating two small vectors vec_1 &lt;- c(1,2,3) vec_2 &lt;- c(4,5,6) # merging them into one big vector big_vec &lt;- c(vec_1, vec_2) Putting a set of vectors together like this flattens them. What we get out of the above is not a vector containing a set of vectors, each containing numbers. Instead, we get out a single vector that contains all of the components of the set of vectors together in the order that we passed them. 2.2.2 Lists A list is a vector comprised of elements of multiple types. # making a list first_list &lt;- c(&quot;hello_world!&quot;, 1) We can turn something into a list explicitly by using list. # making a list more explicitly second_list &lt;- list(&quot;hello_world!&quot;, 1) Lists are very useful in practice because they can be recursive. Elements of lists can be other lists or vectors, R does not flatten lists like it does atomic vectors. Lists are thus useful when we want to store a set of objects, but preserve some internal structure. An example from econometrics, which we will see later, is collecting standard errors for different regression specifications. When we create a regression table, we often want to pass a list of standard errors to tell us how precise our estimates are. Yet, we also want to keep the standard errors of each model together so we do not mix standard errors of different models up. A way we can achieve this is by storing the standard errors for each model in its own vector, and then placing those vectors into a list. The vectors keep the standard errors of the estimates from each model together and separate. We can also flatten lists if we want to by using unlist. Again, we will see how this is useful later. # demonstrating the nested structure in lists # imagine these are the two outputs of standard errors from our variance-covariance # matrix se_1 &lt;- c(0.3, 0.4, 0.5) se_2 &lt;- c(0.4, 0.5, 0.6, 0.7) # now lets create a list with a nested structure se_list &lt;- list(se_1, se_2) # now if we take a look at the list, we can see the nested structure se_list ## [[1]] ## [1] 0.3 0.4 0.5 ## ## [[2]] ## [1] 0.4 0.5 0.6 0.7 # selecting the first or second element of the list will give us # the coresponding vector. # Now, imagine we want to flatten the list i.e remove the nested structure flattened_se_list &lt;- unlist(se_list) flattened_se_list ## [1] 0.3 0.4 0.5 0.4 0.5 0.6 0.7 2.2.3 Working with vectors As mentioned above, vectors have an order. We can use this ordering to select elements from vectors. This is called slicing. We do this by putting square brackets after the name of the vector, containing positions of elements within the vector. You can select a single element of the vector by putting a single number in the brackets that corresponds to the position of that element in the vector. You can select the nth to n+kth element of the vector by passing n:n+k. You can create any combination of those elements by passing the positions of the elements you want to select as a vector. # lets slice the vector we created earlier big_vec[1] ## [1] 1 big_vec[6] ## [1] 6 big_vec[2:3] ## [1] 2 3 big_vec[c(1, 2:3, 6)] ## [1] 1 2 3 6 We can add, subtract, multiply, and divide numbers using +, -, *, /. These also work for vectors. They perform the operations on the vectors elementwise. # Using some basic mathematical operators # on integers/doubles 1+1 ## [1] 2 1-1 ## [1] 0 1*2 ## [1] 2 1/2 ## [1] 0.5 # on vectors big_vec + 1 ## [1] 2 3 4 5 6 7 big_vec - 1 ## [1] 0 1 2 3 4 5 big_vec* 2 ## [1] 2 4 6 8 10 12 big_vec / 2 ## [1] 0.5 1.0 1.5 2.0 2.5 3.0 # notice that each of these returns a vector of results, where each element is # the result of the operation on the element of the previous vector. In practice, you will see many vectors that contain missing values. R denotes missing values with NA. R also has useful commands to find these missing values. is.na() returns a logical vector of the same length as the original vector. Each element corresponds to the element in the original vector. It returns TRUE if this is missing, else FALSE. The any() and all() commands tell us if any or all of the elements of a vector satisfy a logical respectively. The first returns TRUE if at least one of the elements of the vector satisfies the logical, else FALSE. The second returns TRUE if all of the elements of the vector satisfies the logical, else FALSE. We can combine these with is.na() to screen for NAs. # Lets look for some missing values # here&#39;s a vector with a missing value missing_value_vec &lt;- c(1,2,NA,4,5) # now lets see where that missing value is is.na(missing_value_vec) ## [1] FALSE FALSE TRUE FALSE FALSE any(is.na(missing_value_vec) == T) ## [1] TRUE all(is.na(missing_value_vec) == T) ## [1] FALSE Notice that in the middle, we have specified a logical condition using ==. Vectors can also have attributes - metadata that we attach to the vector object. The most important of these to know are names and dimensions. R constructs objects that we store data in like matrices and dataframes as multi-dimensional vectors. 2.3 Functions A function is a map from some arguments to an output. In programming, it takes some things in and performs some operations on that thing. Thus, they are incredibly common. One of the main things we want to do in R is use functions - by taking others from packages and making our own. 2.3.1 Functions from packages A package is a collection of functions that other people have written. Normally, these have a theme. For example, plm is a package of functions for fitting panel data models. Thus, it contains a set of interconnected functions you can use to do lots of things with panel data. Packages are incredibly useful. Using functions from packages is the main way of performing operations in R. Most programmers upload their packages to the Comprehensive R Archive Network or CRAN for short https://cran.r-project.org/. All packages on CRAN have vignettes - a pdf file describing all of the functions that are in the package with examples of how you might use them. To use a package, you first have to download it onto your computer. To do this, you have to run install.packages() with the package name between double quotes within the brackets. Thus, to install plm, we run install.packages(\"plm\") in the console. Once you have a package on your computer, you now have the option to use it in your scripts. To use a package in a script, you need to write library() with the package name before the point you use any functions from the package. If we want to use function from plm, we have to write library(plm) in our script before we use the function. Once we load a package, we can look at the functions in the package using the package explorer on the right in RStudio. Run a function from a package by typing the name of that function. If we have a function from a package and want to easily look up the vignette, we can do this by typing the function name, preceeded by a question mark ?. This brings up the entry from the package vignette on the right hand side of the RStudio viewer. # Some examples of finding the documentation for a specific function # imagine we want to find the vignette for the summary function in base R - # we would run ?summary ## starting httpd help server ... done The output for the inbuilt (or Base R) summary is as below. Imagine here that we had multiple objects called summary from different packages that we might use. We can specify which package we want to select the function from by preceeding the function with the package name and a double colon ::. Our version of summary is from base R - the set of functions that come with the R distribution itself. To call that summary specifically, we can run base::summary. # Some examples of finding the documentation for a specific function ?base::summary # now imagine we want the documentation for the group_by function from the # package dplyr ?dplyr::group_by # notice we can get the documentation without actually loading the package! # But if we wanted to actually run the function we would need to load # the package of course 2.3.2 Defining your own function It is very easy to define functions in R. They are particularly useful in R because they allow you to iterate in a vectorised way by applying a function to a vector. We will see this later. We store functions as objects with the name of the function as the object name. We follow this by the command function(). The arguments to function() are the arguments of the function. We enclose the body of the function in curly brackets. At the end of the function, we specify what we want to return from the function within return(). Once we have our function, we can run it with the name, followed by the value of the arguments within brackets. # lets define our first function - squaring # This takes a numeric argument `x&#39; and returns its # square square_things &lt;- function(x){ y &lt;- x**2 return(y) } # I could have also written this in one line as return(x**2) of course # now lets square something square_things(22) ## [1] 484 square_things(7) ## [1] 49 Of course, we can nest functions if we want to chain some operation and write that in an efficient way. square_things_add_two &lt;- function(x){ y &lt;- square_things(x) + 2 return(y) } square_things_add_two(7) ## [1] 51 Notice that all the objects we define within the function are local - they do not exist in our environment outside of that function. If we try to look for them in the environment pane, they are not there. Thus, if we try to get y outside of the function above, it will return nothing (or worse, something else we have called y that we do not want!). Objects we define outside of functions, by contrast, are global  they exist in the whole environment. Global objects are the ones we can see in our environment pane. 2.4 Iteration Imagine we want to do some operation to lots of things - for example squaring a vector of numbers. Applying an operation to each of those things is called iteration. We can iterate in R using for and while loops as you might have seen in other programming languages. # making a for or while loop in R for (i in range(0,10)){ print(i**2) } ## [1] 0 ## [1] 100 j &lt;- 0 while (j &lt; 10){ print(j**2) j &lt;- j+1 } ## [1] 0 ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 ## [1] 36 ## [1] 49 ## [1] 64 ## [1] 81 If we want to select some variables based on conditions, we can use if and else statements. # if-else x &lt;- 3 if (x==3){ print(&quot;X is three!&quot;) } else { print(&quot;X is not three :(&quot;) } ## [1] &quot;X is three!&quot; R is, however, a vectorised programming language - it is designed to perform operations directly to vectors. Looping does not do this. When we write a loop, we take an object from the set of objects we want to iterate over, apply the operation directly to that object. We then select the next object, apply the operation to that object, and so on. This suggests a quicker and more robust way to iterate - store the objects we want to iterate over as some sort of vector, and perform the operation directly on the entire vector at once. More precisely, we should iterate by designing a function, setting up a vectorised object, and applying the function directly to the vectorised object. In R, we do this using the apply family of functions, and it is called applying. There are four of these functions - sapply, lapply, apply, and tapply. The most common to use are lapply and sapply. lapply takes a list, a function, and a vector of additional arguments to the function. It applies the function to each value of the original list, given the parameters, and returns the list of those values. sapply takes a vector, a function, and a vector of additional arguments to the function. It applies the function to each value of the original vector, given the parameters, and returns the vector of those values. # examples of iteration by applying vec_of_numbers &lt;- c(0:10) squares_vec &lt;- sapply(vec_of_numbers, FUN=square_things) list_of_numbers &lt;- as.list(c(0:10)) sqaures_list &lt;- lapply(list_of_numbers, FUN=square_things) # notice that we can get the same thing as above by flattening the list squares_vec_2 &lt;- unlist(sqaures_list) apply and tapply are less common to use. apply does the same as lapply and sapply above, but takes in and returns a data.frame or matrix instead of a list or vector. We will learn what those objects are in the next and fifth section respectively. tapply takes in a vector containing factor variables and computes a function for another vector by the levels of the factors. This can be very useful for summarising data - for example taking means by groups. applying is much more efficient for most operations than a loop. Of course, for small operations, there is a trade off between run time and developer time. Often it is easier to just write a loop. We cover some more efficient iteration methods in final section if you are interested to learn more. 2.5 Style Now you know how to write some R, it is important to know how to present it in a nice way. Writing readable code is very important for a couple of reasons. Firstly, others might read your code. Secondly, your future self will almost certainly read your code to find some operation you have written before and want to do again. In either case, we want that person to be able to find and interpret it easily. Thus, we need to write code in a consistent and interpretable style. The usual style in R is based on the Google Style guide for R https://google.github.io/styleguide/Rguide.html. We base this section on the synopsis by Hadley Wickham at http://adv-r.had.co.nz/Style.html . Here, I quickly go over the main points. Writing readable code takes practice, but pays huge dividends.As always with style guides, do what we say and not what we do. 2.5.1 File names File names should be meaningful - so if our script does x we should call it do_x.R. If running scripts in order, prefix with numbers e.g 1-do_x.R, 2-do_y.R. 2.5.2 Object names Object names should be lower case - so xyz as opposed to Xyz or XYZ. Separate words in a name with _ - so x_vec as opposed to xvec. Variable names should be nouns, function names should be verbs. Names should be concise but meaningful. Do not name your object the same as an inbuilt object - like list. At best it will confuse your reader. At worst, you can overwrite the inbuilt name in your environment and cause a load of problems for yourself. 2.5.3 Syntax Spacing should be as in English - so spaces after commas, mathematical symbols and so on. The exceptions to this are for colons, variables within brackets - so (x) not ( x ), or if it leads your code to line up in a nicer way. Try to keep code within 80 spaces from the beginning of the line. This means that it will fit on a single sheet of A4 paper if you need to save it to a document (say for a coding test for a job). RStudio places a vertical line in the editor at 80 spaces in that you can use to measure this. When we use curly brackets, the second curly bracket should go on its own line unless it is followed by else. x &lt;- 3 if (x==3){ print(&quot;X is three!&quot;) } else { print(&quot;X is not three :(&quot;) } ## [1] &quot;X is three!&quot; Indent in the same block with two spaces, except when you are indenting an argument within brackets. Then, indent to the beginning of the arguments. very_very_long_function&lt;- function(a= &quot;very_very_very_very_very_long_argument&quot;, b= &quot;another argument&quot;){ return(print(a)) } 2.5.4 Commenting Comment your code! Caring about commenting seems silly, but is actually very very important. It is very hard to interpret someone elses or your own past code without commenting. Comment frequently using the #. Comments should explain what the function or line of code is doing. # now lets create a variable that stores the string &quot;hello_world&quot; hello_world_string &lt;- &quot;hello_world&quot; # lets print it hello_world_string ## [1] &quot;hello_world&quot; Break scripts into sections, and delimit these sections with # followed by a name saying what the section is doing, followed by --- up to the line in the IDE. This best allows you to easily find the area of the code you want. A common thing to do is to first have a section where you read in the data, then one containing all of your functions, and then one where you run your models (which you may or may not break into further sections. At the beginning of a script, add a line explaining what the code is doing and the name of the person who made it. # Example section breaks for an assignment with a series of questions # Code for assignment 1 # Your group name rm(list=ls()) # reading in data ------------------------------------------------------------- # code for reading in our data # functions -------------------------------------------------------------------- # whatever functions we want to use # question 1 ------------------------------------------------------------------- # code where we compute what we need for question 1 # question 2 ------------------------------------------------------------------- # code where we compute what we need for question 2 "],["reading-analysing-and-manipulating-data.html", "Chapter 3 Reading, analysing, and manipulating data 3.1 Reading in datasets as dataframes 3.2 Slicing dataframes 3.3 Basic summary statistics 3.4 Summarising with the tidyverse - dplyr and ggplot2", " Chapter 3 Reading, analysing, and manipulating data Here, we cover how to read in data, different ways of storing data, how to plot it, and how to manipulate it. First, we start with the basics - setting up a data frame (like an excel sheet or pandas dataframe) and a matrix. Second, we show how to select and use subsets of your the data based on conditions. Third, we present some basic ways to summarise your data - summary tables and plots. We finish by taking a quick look at some more advanced objects called data tables, and taking a quick look at the packages in the tidyverse, which are designed to allow you to quickly ad easily split, apply, and recombine data. For illustration, we use a real life dataset from Dell and Querubin (2018) Nation Building Through Foreign Intervention: Evidence from Discontinuities in Military Strategies. This paper measures the effect of increased US firepower on support for insurgents in South Vietnam during the Vietnam war. In this section, we simply manipulate the data. In the section on modelling, we look at some relationships in this data. 3.1 Reading in datasets as dataframes The basic way to work with a dataset in R is to use an object called a dataframe. At its core, a dataframe is like a table where each column is a vector. It is the R equivalent to the pandas objects in Python. Intuitively, it looks a bit like a spreadsheet with column headers. To read in a file as a given data type, first you need to read the file. Typically, data will come in csv files. The command to read them is read.csv. There are equivalents for xls and dta files. # lets read in a new dataframe # To download the original data, go to # https://scholar.harvard.edu/files/dell/files/nationbuilding.pdf # The file we will use is there as `firstclose_post.dta&#39; # read.csv allows us to read a csv file into R. The `as.data.frame&#39; wrapper # tells R that we want the object to be represented as a dataframe df &lt;- as.data.frame(read.csv(&quot;vietnam_war.csv&quot;, stringsAsFactors = F)) One thing that can be unintuitive is how this treats strings. By default, R assumes that any strings in your dataset are factors, so will turn them into a factor variable. This can cause a lot of weird problems when trying to slice. So I recommend starting by setting StringsAsFactors==F, and then manually changing strings back into factors if you need them. If you ever wonder why the default is T, see https://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/. Once you have read the data, use as followed by the name of your data type to put it in the data type you want. as.data.frame has a series of different arguments allowing you to set the column names, the row names, and so on. A lot of older economists still use Stata. In Stata, you have to store the data files in a format called a .dta file. To read these into R, use the read_dta command in the `haven package. # we explain how to read in packages in the next section - if you need to # read a .dta file you should go and look at that section first # library(haven) # df &lt;- as.data.frame(read_dta(&quot;firstclose_post.dta&quot;)) Stata can be as simple as R for small operations, but can get very messy very quickly if you have to do anything more complex. To see this in practice, go and have a look at how many files Dell and Querubin have to use to compute the estimates in their paper. We can have a quick look at the top rows in our dataframe using head. # looking at the top of the dataframe # only selecting the first ten columns for ease of reading # code to look at all of them would be head(df) head(df[,c(1:10)]) ## X usid fr_strikes_mean fr_forces_mean pop_g naval_attack ## 1 0 101010101 0.6000000 0.6666667 -0.7473872 0 ## 2 1 101010102 0.3703704 0.5555556 -0.4082199 0 ## 3 2 101010103 0.3703704 0.5555556 -0.3186806 0 ## 4 3 101010104 0.3750000 0.5000000 -0.5415232 0 ## 5 4 101010105 0.3703704 0.5555556 -0.4552453 0 ## 6 5 101010201 0.7222222 1.0000000 -1.7762434 NA ## sh_pf_presence sh_rf_presence fr_init fw_init ## 1 0.00000000 0.00000000 0.00000000 0 ## 2 0.96296300 0.00000000 0.03703704 0 ## 3 0.03703704 0.07407408 0.03703704 0 ## 4 0.12500000 0.00000000 0.00000000 0 ## 5 0.66666670 0.00000000 0.03703704 0 ## 6 0.50000000 0.00000000 0.00000000 0 Find the column names using colnames. # what are the columns called? # Again only printing first ten for ease - code for all would be # colnames(df) colnames(df)[1:10] ## [1] &quot;X&quot; &quot;usid&quot; &quot;fr_strikes_mean&quot; &quot;fr_forces_mean&quot; ## [5] &quot;pop_g&quot; &quot;naval_attack&quot; &quot;sh_pf_presence&quot; &quot;sh_rf_presence&quot; ## [9] &quot;fr_init&quot; &quot;fw_init&quot; # lets read in a new dataframe # read.csv allows us to read a csv file into R. The `as.data.frame&#39; wrapper # tells R that we want the object to be represented as a dataframe # For illustration, we are going to use some of the data from Dell and Querubin (2017) # `Nation building through foreign interventions&#39;. This estimates the causal effect of # military firepower on insurgent support on the intensive margin # discontinuities in US strategies across regions in South Vietnam during their # occupation. We have observations by hamlet # To download the original data, go to # https://scholar.harvard.edu/files/dell/files/nationbuilding.pdf # The file we will use is there as `firstclose_post.dta&#39; df &lt;- as.data.frame(read.csv(&quot;vietnam_war.csv&quot;, stringsAsFactors = F)) # a lot of older economists still use Stata. In Stata, you have to store # the data files in a format called .dta file. To read these into R, use the # read_dta command in the `haven&#39; package # library(haven) # df &lt;- as.data.frame9(read_dta(&quot;firstclose_post.dta&quot;)) # looking at the top of the dataframe # only selecting the first ten columns for ease of reading # code to look at all of them would be head(df) head(df[,c(1:10)]) ## X usid fr_strikes_mean fr_forces_mean pop_g naval_attack ## 1 0 101010101 0.6000000 0.6666667 -0.7473872 0 ## 2 1 101010102 0.3703704 0.5555556 -0.4082199 0 ## 3 2 101010103 0.3703704 0.5555556 -0.3186806 0 ## 4 3 101010104 0.3750000 0.5000000 -0.5415232 0 ## 5 4 101010105 0.3703704 0.5555556 -0.4552453 0 ## 6 5 101010201 0.7222222 1.0000000 -1.7762434 NA ## sh_pf_presence sh_rf_presence fr_init fw_init ## 1 0.00000000 0.00000000 0.00000000 0 ## 2 0.96296300 0.00000000 0.03703704 0 ## 3 0.03703704 0.07407408 0.03703704 0 ## 4 0.12500000 0.00000000 0.00000000 0 ## 5 0.66666670 0.00000000 0.03703704 0 ## 6 0.50000000 0.00000000 0.00000000 0 # what are the columns called? # Again only printing first ten for ease - code for all would be # colnames(df) colnames(df)[1:10] ## [1] &quot;X&quot; &quot;usid&quot; &quot;fr_strikes_mean&quot; &quot;fr_forces_mean&quot; ## [5] &quot;pop_g&quot; &quot;naval_attack&quot; &quot;sh_pf_presence&quot; &quot;sh_rf_presence&quot; ## [9] &quot;fr_init&quot; &quot;fw_init&quot; # we will mainly use the variable fr_strikes_mean - the average number of # months in the given quarter with at least one airstrike # lets add a column to a dataframe - the square of foreign airstrikes df[[&quot;fr_strikes_mean_squared&quot;]] &lt;- df[[&quot;fr_strikes_mean&quot;]]**2 # say the new variable is a vector vec - then we have to create it as a vector # first and then add it as so #vec &lt;- c(1:length(df[,1])) #df[[&quot;new_variable&quot;]] &lt;- vec # lets check it actually is there vec &lt;- df[[&quot;fr_strikes_mean&quot;]]**2 # and see if we produced any # we can bind on new columns or rows to the outside using cbind and rbind df &lt;- cbind(df, vec) #df &lt;- rbind(df, vec) # we can rename a column by position using `names(name of dataframe)[number of column]&#39; names(df)[1] &lt;- &quot;X&quot; # lets create a dataframe from two variables using cbind # creating two vectors of the same length vec &lt;- c(1:10) vec2 &lt;- c(11:20) # now, if they are the same length, we can cbind them together, # then use as.data.frame to turn them into a dataframe. We can pass arguments # such as column names etc directly to this - see the documentation for # data frames for more information df_2 &lt;- as.data.frame(cbind(vec, vec2), columns=c(&quot;var1&quot;, &quot;var2&quot;)) 3.2 Slicing dataframes The most useful thing to know about dataframes is how to slice them. This is how we extract the data we want to use in regression models etc. Dataframes behave as we might expect, knowing that they are made up of linked vectors. The syntax for slicing is df[row, column]. We can slice by position, as for vectors above, or by passing vectors of column names (e.g column names). If we want to select all rows/columns, we leave the appropriate entry blank. To extract a single column as a vector, we use double square brackets containing the column name as a string. # slicing sl_1 &lt;- df[,1] sl_2 &lt;- df[c(3:4), c(1,2)] sl_3 &lt;- df[,c(&quot;usid&quot;, &quot;fr_forces_mean&quot;)] # lets find a single column sl_4 &lt;- df[[&quot;yr&quot;]] # you might also see people do this with a dollar sign like so #df$yr # but apparently this is less robust to exceptions. If you are more interested # in this, see the section on slicing in `Advanced R&#39; by Hadley Wickham. Including square brackets after the syntax above allows us to add additional conditions for slicing. This can be very powerful, as we can select columns and then use logical conditions to slice our dataframe based on the values of the columns. # Looking only at the hamlets that are buddhist df_budd &lt;- df[df[[&quot;buddhist&quot;]]==1,] # Only the hamlets that had less than half of months with at least one airstrike # in quarters in 1971 df_2 &lt;- df[df[[&quot;fr_strikes_mean&quot;]]&lt;0.5 &amp; df[[&quot;yr&quot;]]==1971,] # Now any hamlets that existed in 1971 or had less than half of months with one airstrike df_3 &lt;- df[df[[&quot;fr_strikes_mean&quot;]]&lt;0.5 | df[[&quot;yr&quot;]]==1971,] # lets extract the mean number of months with foreign force engagments in those hamlets using a square # bracket after we select the vector. Note, we are only passing a logical in the # final bracket, so we omit the comma that indicates the dimension # above forces &lt;- df[[&quot;fr_forces_mean&quot;]][df[[&quot;fr_strikes_mean&quot;]]&lt;0.5 &amp; df[[&quot;yr&quot;]]==1971] # This returns the entries of the vector fr_forces_mean, where the corresponding # logicals are fulfilled. Thus, we get a vector of entries. Now lets take the # mean of this. mean(forces) ## [1] 0.3698262 3.3 Basic summary statistics Once we have our dataset, we should summarise it. This allows us to eyeball any trends, see any patterns or weird features that we might have missed, and so on. These are common in empirical applications. In the worst case, it can help uncover fraud - e.g https://datacolada.org/98. Here, we will show you how to create a summary table and convert it to tex code, plot a histogram, and a line chart. The most basic way to summarise your data is to use the base `summary command. This is a generic way of summarising many different R objects. Passing a dataframe of numerical variables gives the quantiles and mean of each column, as well as the number of NAs. Passing factors gives a frequency table of the frequency of each factor. library(stargazer) ## ## Please cite as: ## Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. ## R package version 5.2.2. https://CRAN.R-project.org/package=stargazer # Lets summarise the first twelve columns of our dataset df_for_summary &lt;- df[,c(1:12)] sum_1 &lt;- summary(df_for_summary) To do this in a more flexible way, we can apply summary functions to the data using sapply'.sapply is a member of the `apply family of functions that takes as an input a dataframe or matrix and iterates over the columns. It returns a dataframe or matrix object depending which we passed it. # computing the mean of each column, omitting any NA observations sapply(df_for_summary, FUN=mean, na.rm=T) ## X usid fr_strikes_mean fr_forces_mean pop_g ## 6.103000e+03 2.999219e+08 2.623032e-01 4.394916e-01 -2.679248e-02 ## naval_attack sh_pf_presence sh_rf_presence fr_init fw_init ## 3.376539e-03 3.217444e-01 7.901538e-02 3.582017e-01 1.535283e-02 ## en_d fw_d ## 8.612760e+00 6.300729e-02 sapply(df_for_summary, FUN=sd, na.rm=T) ## X usid fr_strikes_mean fr_forces_mean pop_g ## 3.524002e+03 1.258183e+08 3.073952e-01 3.249269e-01 1.996891e-01 ## naval_attack sh_pf_presence sh_rf_presence fr_init fw_init ## 2.358882e-02 3.231069e-01 1.785427e-01 3.131460e-01 5.263280e-02 ## en_d fw_d ## 3.879940e+01 2.004662e-01 Neither of these look very nice however. The package stargazer allows us to create nice looking tables in R that we can save or export. It can even output the table as a tex file, that we can use directly in our Latex documents. This package contains the function stargazer, which is a general function we can use to create nice-looking summary and regression tables. It has a lot of arguments we can use to create tables in different formats, including different things etc. We change what summary statistics we display by passing a vector of their names as characters to the summary.stat argument. stargazer(df_for_summary, type=&quot;html&quot;) ## ## &lt;table style=&quot;text-align:center&quot;&gt;&lt;tr&gt;&lt;td colspan=&quot;8&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Statistic&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;Mean&lt;/td&gt;&lt;td&gt;St. Dev.&lt;/td&gt;&lt;td&gt;Min&lt;/td&gt;&lt;td&gt;Pctl(25)&lt;/td&gt;&lt;td&gt;Pctl(75)&lt;/td&gt;&lt;td&gt;Max&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td colspan=&quot;8&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;X&lt;/td&gt;&lt;td&gt;12,207&lt;/td&gt;&lt;td&gt;6,103.000&lt;/td&gt;&lt;td&gt;3,524.002&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;3,051.5&lt;/td&gt;&lt;td&gt;9,154.5&lt;/td&gt;&lt;td&gt;12,206&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;usid&lt;/td&gt;&lt;td&gt;12,207&lt;/td&gt;&lt;td&gt;299,921,922.000&lt;/td&gt;&lt;td&gt;125,818,263.000&lt;/td&gt;&lt;td&gt;101,010,101&lt;/td&gt;&lt;td&gt;207,082,903.0&lt;/td&gt;&lt;td&gt;434,030,755.0&lt;/td&gt;&lt;td&gt;492,010,602&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;fr_strikes_mean&lt;/td&gt;&lt;td&gt;12,207&lt;/td&gt;&lt;td&gt;0.262&lt;/td&gt;&lt;td&gt;0.307&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.444&lt;/td&gt;&lt;td&gt;1.000&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;fr_forces_mean&lt;/td&gt;&lt;td&gt;12,207&lt;/td&gt;&lt;td&gt;0.439&lt;/td&gt;&lt;td&gt;0.325&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.167&lt;/td&gt;&lt;td&gt;0.708&lt;/td&gt;&lt;td&gt;1.000&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;pop_g&lt;/td&gt;&lt;td&gt;11,967&lt;/td&gt;&lt;td&gt;-0.027&lt;/td&gt;&lt;td&gt;0.200&lt;/td&gt;&lt;td&gt;-6.853&lt;/td&gt;&lt;td&gt;-0.008&lt;/td&gt;&lt;td&gt;0.018&lt;/td&gt;&lt;td&gt;4.520&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;naval_attack&lt;/td&gt;&lt;td&gt;11,545&lt;/td&gt;&lt;td&gt;0.003&lt;/td&gt;&lt;td&gt;0.024&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.500&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;sh_pf_presence&lt;/td&gt;&lt;td&gt;12,146&lt;/td&gt;&lt;td&gt;0.322&lt;/td&gt;&lt;td&gt;0.323&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.625&lt;/td&gt;&lt;td&gt;1.000&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;sh_rf_presence&lt;/td&gt;&lt;td&gt;12,146&lt;/td&gt;&lt;td&gt;0.079&lt;/td&gt;&lt;td&gt;0.179&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.037&lt;/td&gt;&lt;td&gt;1.000&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;fr_init&lt;/td&gt;&lt;td&gt;12,200&lt;/td&gt;&lt;td&gt;0.358&lt;/td&gt;&lt;td&gt;0.313&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.091&lt;/td&gt;&lt;td&gt;0.600&lt;/td&gt;&lt;td&gt;1.000&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;fw_init&lt;/td&gt;&lt;td&gt;12,200&lt;/td&gt;&lt;td&gt;0.015&lt;/td&gt;&lt;td&gt;0.053&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.556&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;en_d&lt;/td&gt;&lt;td&gt;12,200&lt;/td&gt;&lt;td&gt;8.613&lt;/td&gt;&lt;td&gt;38.799&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.500&lt;/td&gt;&lt;td&gt;5.547&lt;/td&gt;&lt;td&gt;923.667&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;fw_d&lt;/td&gt;&lt;td&gt;12,200&lt;/td&gt;&lt;td&gt;0.063&lt;/td&gt;&lt;td&gt;0.200&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.000&lt;/td&gt;&lt;td&gt;0.037&lt;/td&gt;&lt;td&gt;7.000&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td colspan=&quot;8&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt; # now lets just have a look at the mean and sd stargazer(df_for_summary, type=&quot;html&quot;, summary.stat = c(&quot;mean&quot;, &quot;sd&quot;)) ## ## &lt;table style=&quot;text-align:center&quot;&gt;&lt;tr&gt;&lt;td colspan=&quot;3&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Statistic&lt;/td&gt;&lt;td&gt;Mean&lt;/td&gt;&lt;td&gt;St. Dev.&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td colspan=&quot;3&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;X&lt;/td&gt;&lt;td&gt;6,103.000&lt;/td&gt;&lt;td&gt;3,524.002&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;usid&lt;/td&gt;&lt;td&gt;299,921,922.000&lt;/td&gt;&lt;td&gt;125,818,263.000&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;fr_strikes_mean&lt;/td&gt;&lt;td&gt;0.262&lt;/td&gt;&lt;td&gt;0.307&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;fr_forces_mean&lt;/td&gt;&lt;td&gt;0.439&lt;/td&gt;&lt;td&gt;0.325&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;pop_g&lt;/td&gt;&lt;td&gt;-0.027&lt;/td&gt;&lt;td&gt;0.200&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;naval_attack&lt;/td&gt;&lt;td&gt;0.003&lt;/td&gt;&lt;td&gt;0.024&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;sh_pf_presence&lt;/td&gt;&lt;td&gt;0.322&lt;/td&gt;&lt;td&gt;0.323&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;sh_rf_presence&lt;/td&gt;&lt;td&gt;0.079&lt;/td&gt;&lt;td&gt;0.179&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;fr_init&lt;/td&gt;&lt;td&gt;0.358&lt;/td&gt;&lt;td&gt;0.313&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;fw_init&lt;/td&gt;&lt;td&gt;0.015&lt;/td&gt;&lt;td&gt;0.053&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;en_d&lt;/td&gt;&lt;td&gt;8.613&lt;/td&gt;&lt;td&gt;38.799&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;fw_d&lt;/td&gt;&lt;td&gt;0.063&lt;/td&gt;&lt;td&gt;0.200&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td colspan=&quot;3&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt; # Lets flip the axes stargazer(df_for_summary, type=&quot;html&quot;, summary.stat = c(&quot;mean&quot;, &quot;sd&quot;), flip = T) ## ## &lt;table style=&quot;text-align:center&quot;&gt;&lt;tr&gt;&lt;td colspan=&quot;13&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Statistic&lt;/td&gt;&lt;td&gt;X&lt;/td&gt;&lt;td&gt;usid&lt;/td&gt;&lt;td&gt;fr_strikes_mean&lt;/td&gt;&lt;td&gt;fr_forces_mean&lt;/td&gt;&lt;td&gt;pop_g&lt;/td&gt;&lt;td&gt;naval_attack&lt;/td&gt;&lt;td&gt;sh_pf_presence&lt;/td&gt;&lt;td&gt;sh_rf_presence&lt;/td&gt;&lt;td&gt;fr_init&lt;/td&gt;&lt;td&gt;fw_init&lt;/td&gt;&lt;td&gt;en_d&lt;/td&gt;&lt;td&gt;fw_d&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td colspan=&quot;13&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Mean&lt;/td&gt;&lt;td&gt;6,103.000&lt;/td&gt;&lt;td&gt;299,921,922.000&lt;/td&gt;&lt;td&gt;0.262&lt;/td&gt;&lt;td&gt;0.439&lt;/td&gt;&lt;td&gt;-0.027&lt;/td&gt;&lt;td&gt;0.003&lt;/td&gt;&lt;td&gt;0.322&lt;/td&gt;&lt;td&gt;0.079&lt;/td&gt;&lt;td&gt;0.358&lt;/td&gt;&lt;td&gt;0.015&lt;/td&gt;&lt;td&gt;8.613&lt;/td&gt;&lt;td&gt;0.063&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;St. Dev.&lt;/td&gt;&lt;td&gt;3,524.002&lt;/td&gt;&lt;td&gt;125,818,263.000&lt;/td&gt;&lt;td&gt;0.307&lt;/td&gt;&lt;td&gt;0.325&lt;/td&gt;&lt;td&gt;0.200&lt;/td&gt;&lt;td&gt;0.024&lt;/td&gt;&lt;td&gt;0.323&lt;/td&gt;&lt;td&gt;0.179&lt;/td&gt;&lt;td&gt;0.313&lt;/td&gt;&lt;td&gt;0.053&lt;/td&gt;&lt;td&gt;38.799&lt;/td&gt;&lt;td&gt;0.200&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td colspan=&quot;13&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt; # Lets create some tex code to use in our tex files, and save it as a tex file stargazer(df_for_summary,out = &quot;summary_stats.tex&quot;,summary.stat = c(&quot;mean&quot;, &quot;median&quot;, &quot;sd&quot;), title = &quot;Partial summary statistics - Dell and Querubin (2018)&quot;, digits = 2, float.env = &quot;table&quot;, notes = &quot;&quot;) ## ## % Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu ## % Date and time: Mon, Oct 04, 2021 - 18:56:58 ## \\begin{table}[!htbp] \\centering ## \\caption{Partial summary statistics - Dell and Querubin (2018)} ## \\label{} ## \\begin{tabular}{@{\\extracolsep{5pt}}lccc} ## \\\\[-1.8ex]\\hline ## \\hline \\\\[-1.8ex] ## Statistic &amp; \\multicolumn{1}{c}{Mean} &amp; \\multicolumn{1}{c}{Median} &amp; \\multicolumn{1}{c}{St. Dev.} \\\\ ## \\hline \\\\[-1.8ex] ## X &amp; 6,103.00 &amp; 6,103 &amp; 3,524.00 \\\\ ## usid &amp; 299,921,922.00 &amp; 325,010,608 &amp; 125,818,263.00 \\\\ ## fr\\_strikes\\_mean &amp; 0.26 &amp; 0.13 &amp; 0.31 \\\\ ## fr\\_forces\\_mean &amp; 0.44 &amp; 0.39 &amp; 0.32 \\\\ ## pop\\_g &amp; $-$0.03 &amp; 0.002 &amp; 0.20 \\\\ ## naval\\_attack &amp; 0.003 &amp; 0.00 &amp; 0.02 \\\\ ## sh\\_pf\\_presence &amp; 0.32 &amp; 0.23 &amp; 0.32 \\\\ ## sh\\_rf\\_presence &amp; 0.08 &amp; 0.00 &amp; 0.18 \\\\ ## fr\\_init &amp; 0.36 &amp; 0.26 &amp; 0.31 \\\\ ## fw\\_init &amp; 0.02 &amp; 0.00 &amp; 0.05 \\\\ ## en\\_d &amp; 8.61 &amp; 1.79 &amp; 38.80 \\\\ ## fw\\_d &amp; 0.06 &amp; 0.00 &amp; 0.20 \\\\ ## \\hline \\\\[-1.8ex] ## \\multicolumn{4}{l}{} \\\\ ## \\end{tabular} ## \\end{table} We can create a histogram of numerical variables by using the base R hist command. We have to make sure to specify a number of bins when we do this, using the breaks argument. # Lets look at the distribution of the number of mean monthly airstrikes across # South Vietnamese hamlets in 1971 airstrikes &lt;- df[[&quot;fr_strikes_mean&quot;]][df[[&quot;yr&quot;]]==1971] hist(airstrikes, breaks=20, col=&quot;lightblue&quot;, main=&quot;Mean months with scheduled airstrikes on South Vietnamese hamlets, 1971&quot;) We can also plot any two variables against each other by using the base plot command. The first argument is the variable on the x axis, the second argument is the variable on the y axis. The type argument allows you to specify whether you want a scatterplot, a line plot, or other types of plot. Additional arguments allow us to specify the size of the points, the colour, axis sizes, titles, and so on. We can make more complex plots by creating a plot object, and then building on top of it. I give an example of a more complex plot below. We start with a single plot, containing a single line. We also specify some labels, a title, and control the size of the text in general and axis labels in particular (using the cex arguments). This is the first line of code in this base plot. Next, we build it up a single element at a time, adding a new line of code below for each element. We first plot two additional lines with line. We then manually control each of the axes with axis. Passing the xact=\"n\" and yact=\"n\" commands to the first plot allow us to do this. # plotting the relationship between mean foreign forces and mean airstrikes in # a hamlet over all years plot(df[[&quot;fr_forces_mean&quot;]], df[[&quot;fr_strikes_mean&quot;]], type = &quot;p&quot;, col=&quot;lightblue&quot;, pch = 19, xlab = &quot;Mean months with foreign forces&quot;, ylab = &quot;Mean months with airstrikes&quot;, main= &quot;Mean months with foreign forces vs. mean months with scheduled airstrikes by quarter&quot;) # now lets plot the mean airstrikes, ground operations, and naval strikes by year # this will illustrate the basics of how to make more complex plots with base R # Also this is a nice illustration how to use functions in lappply # inputs: list of unique times in the dataframe, # dataframe with at least two columns - time and thing we want to summarise, # name of column representing units of time in the dataframe, and # name of column we want to summarise # outputs: list of means of column by year mean_by_year &lt;- function(time, df, name_of_year_column, name_of_summary_column){ obs_by_year &lt;- df[[name_of_summary_column]][df[[name_of_year_column]]==time] return(mean(obs_by_year, na.rm=T)) } times_list &lt;- as.list(unique(df[[&quot;yr&quot;]])) # lapply automatically assumes the list is the first argument to the function # thus, we need to specify the other arguments as arguments to lapply mean_airstrikes &lt;- lapply(times_list, FUN=mean_by_year, df=df, name_of_year_column=&quot;yr&quot;, name_of_summary_column = &quot;fr_strikes_mean&quot;) # this gives us a list - we need to flatten it into a vector to plot it # unlist does this mean_airstrikes &lt;- unlist(mean_airstrikes) mean_troops &lt;- lapply(times_list, FUN=mean_by_year, df=df, name_of_year_column=&quot;yr&quot;, name_of_summary_column = &quot;fr_forces_mean&quot;) mean_troops &lt;- unlist(mean_troops) # now plotting the airstrikes by year # note a common trap here! when we unlist the times_list, it comes out both unordered # and as a factor vector! So, if we try to plot it just like this, we get an error. # Instead, we need to unlist it, turn it into numbers, and then order the vector. times &lt;- sort(unique(df[[&quot;yr&quot;]])) plot(times, mean_airstrikes, col=&quot;red&quot;, type=&quot;l&quot;, xlab=&quot;Year&quot;, ylab=&quot;Engagements&quot;, main = &quot;Mean monthly engagements in South Vietnamese hamlets by type&quot;, cex = 1, cex.lab = 1.25, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylim = c(0.25, 0.5)) axis(2, at = seq(0, 1, by = 0.1), las = 1, cex.axis = 1.25) axis(1, at = seq(1970, 1972, by = 1), cex.axis = 1.25) lines(times, mean_troops, col=&quot;blue&quot;) axis(2, at = seq(0, 1200, by = 100), las = 1, cex.axis = 1.25) legend(&quot;topleft&quot;, legend = c(&quot;Mean airstrikes per month&quot;, &quot;Mean troop engagements per month&quot;), pch=19, col = c(&quot;red&quot;, &quot;blue&quot;), cex = 1) We can save any plot as a png file automatically by wrapping it in png() and dev.off as above. 3.4 Summarising with the tidyverse - dplyr and ggplot2 The tidyverse system of libraries can make summarising and plotting data very easy - especially if it involves grouping the data. They also make complex operations more easily readable. It does this by adopting a different grammar to how standard R looks, and by being opinionated - i.e making automatic decisions about a lot of defaults. The downside to this is that you have to be careful - the defaults can be different to what you would want - and you as the programmer have less control. This is only a very very brief introduction to some tidyverse functions. If you like them, I would strongly recommend Hadley Wickhams book R for data science, which he has put in web form here https://r4ds.had.co.nz/. He is the creator of the tidyverse; and his books are really thorough, accessible, and well written. 3.4.1 Dplyr There are six key dplyr functions: filter to select observations by values, arrange to reorder rows, select to fetch columns by name, mutate to create new columns from existing ones, group_by to fetch the values in one column that depend on another, and summarise to compute summary statistics. filter selects all the rows in a dataframe that satisfy one or more conditions. The first argument is the dataframe; the subsequent arguments are logical conditions. You do not need to put column names in quotes in this, and all, tidyverse functions. It returns a dataframe with only rows that satisfy this condition. arange orders a dataframe in descending order by the values in a set of columns, in the order that we pass the column names. The first argument is the dataframe. The subsequent arguments are the column names. It returns an ordered dataframe. select subsets a dataframe by column names. It is another way of doing the slicing we did in base R above. The first argument is the dataframe. The subsequent arguments are the column names we want to select. It returns a dataframe containing only those columns. mutate creates new columns that are functions of existing columns. It is another way of doing the slicing and tranformation we did above. The first argument is the dataframe. The subsequent arguments are the columns we want to create. It returns a dataframe that is the original dataframe plus the columns we wanted to create. summarise creates a table of summary statistics. This is similar to the summary command in base R. group_by, however, allows us to create summaries that depend on the values of certain columns e.g mean airstrikes by year. This allows us to easily get at more complex features of our data. When we use tidyverse packages, we can chain operations using a pipe %&gt;%. In words, this command says and then. This can be very powerful, as it allows us to do lots of complex operations to dataframes in single sets of commands as opposed to creating lots of intermediate objects, or using messier nested functions. # filter selects all the v # first argument is the dataframe, second argument is the column value you want # to filter by library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.0.4 v dplyr 1.0.2 ## v tidyr 1.1.2 v stringr 1.4.0 ## v readr 1.4.0 v forcats 0.5.0 ## Warning: package &#39;ggplot2&#39; was built under R version 4.0.5 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() filtered_df &lt;- filter(df, yr==1972) arranged_df &lt;- arrange(df, yr, usid) selected_df &lt;- select(df, usid, yr, fr_forces_mean) mutated_df &lt;- mutate(df, fr_strikes_mean_squared = fr_strikes_mean**2) grouped_df &lt;- group_by(df, yr) summarise(grouped_df, count=n(), mean_forces = mean(fr_forces_mean)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 3 x 3 ## yr count mean_forces ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1970 8571 0.441 ## 2 1971 2989 0.429 ## 3 1972 647 0.464 # Combining both of these to do some powerful summarising df %&gt;% select(usid, yr, fr_forces_mean, fr_strikes_mean, naval_attack) %&gt;% group_by(as.factor(yr)) %&gt;% summarise(count=n(), mean_forces = mean(fr_forces_mean), mean_airstrikes = mean(fr_strikes_mean), mean_naval_attack = mean(naval_attack)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 3 x 5 ## `as.factor(yr)` count mean_forces mean_airstrikes mean_naval_attack ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1970 8571 0.441 0.255 NA ## 2 1971 2989 0.429 0.269 NA ## 3 1972 647 0.464 0.327 NA ggplot2 is the most common way to make plots in R. It is an easier way to make beautiful plots than using the base plot and hist functions. ggplot2 uses its own syntax, called the grammar of graphics. In short, you create a base plot object, and then add additional things to it using + signs afterwards. In any ggplot, we start by passing ggplot(). This sets up the coordinate system for what we want to summarise. Next, we pass how we want to summarise it. This can be in a lineplot, scatterplot, heatplot and so on. Each have their own ggplot2 commands. Within that, or the ggplot() command, we have to specify a mapping from the values in our dataset to the things on the plot. We do this with the mapping argument. We always pair this with an aes wrapper, within which we specify the aesthetics for this plot - what is on the x axis, what is on the y axis, and some other possible arguments. We can use these to colour the points by characteristics, put them into different shapes, and so on as long as these things map from the values of the points. There are other arguments, such as general size colour, that are properties of the points but not of their values. These go outside of the mapping argument. This is the base ggplot uses to build the plots. Next, we can add arguments and layer up commands to build the type of the plot we want to make. There are many different ways that we can do this to make some incredibly clean and beautiful plots. For examples, see http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html#Histogram. # Lets start with the simplest thing - a scatter plot ggplot(data = df[df[[&quot;yr&quot;]]==1972,]) + geom_point(mapping = aes(x =fr_strikes_mean, y=en_prop)) ## Warning: Removed 21 rows containing missing values (geom_point). # lets change the colour of all of the points ggplot(data = df[df[[&quot;yr&quot;]]==1972,]) + geom_point(mapping = aes(x =fr_strikes_mean, y=en_prop), colour=&quot;orange&quot;) ## Warning: Removed 21 rows containing missing values (geom_point). # now lets colour some points ggplot(data = df[df[[&quot;yr&quot;]]==1972,]) + geom_point(mapping = aes(x =fr_strikes_mean, y=en_prop, color = fr_forces_mean)) ## Warning: Removed 21 rows containing missing values (geom_point). # we could also do this with, for example, size ggplot(data = df[df[[&quot;yr&quot;]]==1972,]) + geom_point(mapping = aes(x =fr_strikes_mean, y=en_prop, size = fr_forces_mean)) ## Warning: Removed 21 rows containing missing values (geom_point). # now lets build up by adding a smoothed line through all of the points ggplot(data = df[df[[&quot;yr&quot;]]==1972,], mapping = aes(x =fr_strikes_mean, y=en_prop)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## Warning: Removed 21 rows containing non-finite values (stat_smooth). ## Warning: Removed 21 rows containing missing values (geom_point). "],["estimating-models.html", "Chapter 4 Estimating models 4.1 Modelling with functions 4.2 Matrix algebra", " Chapter 4 Estimating models Now we know how to deal with data, we can think about fitting some models! There are two main ways to do this in R: using routines in packages, or programming the mathematical operations yourself using matrices. Both have their upsides and downsides. R is very good at the first, and still very decent - though not the best language out there - for the second. The first is typically a lot easier. The main upside of R is its amazing array of different packages containing almost every model you would like to use, with lots of options and community support. The downside is that you have less control. Someone else has decided what summary statistics to report, which optimiser to use, how to deal with NAs and so on. These are not always what you would like. We should note, however, that even the package functions in R are typically much more flexible than alternatives like Stata. It is much easier to work out, and change, what is going on under the hood. This makes R a great choice for applied statistics, econometrics, and data science, and is part of why R is so popular in these communities. The second is typically a bit harder to do. You have to know how to program the models, all your desired summary statistics etc yourself. The upside is that you can choose exactly what you want to do and how to do it. Also, R is less optimised for matrix operations than dedicated high-performance computing languages like Julia, or Python 3 with Numba. Thus, if you only want to program your own estimators and never use packages, maybe consider these instead (see https://julialang.org/ or https://numba.pydata.org/) We are going to show you how to fit estimators using functions first. We will introduce is using the lm() function for linear regression that comes inbuilt in R, and sandwich package for computing different covariance matrices. Through it, we are going to encounter some useful tips and tricks for using functions from packages. Afterwards, we will deal with programming your own estimators using matrix programming. 4.1 Modelling with functions 4.1.1 Linear regression with lm() The lm function is how we perform OLS in R. Most of the models you will encounter in R packages are built off of this base function. Through it, we are going to encounter some useful tips and tricks for using inbuilt functions. Thus, there are high returns to knowing how it works and reports summary statistics. In the workflow section, we will see how to automatically make nice latex regression tables from lm objects and sandwich standard errors with stargazer(). # lets read in a new dataframe # read.csv allows us to read a csv file into R. The `as.data.frame&#39; wrapper # tells R that we want the object to be represented as a dataframe # For illustration, we are going to use some of the data from Dell and Querubin (2017) # `Nation building through foreign interventions&#39;. This estimates the causal effect of # military firepower on insurgent support on the intensive margin # discontinuities in US strategies across regions in South Vietnam during their # occupation. We have observations by hamlet # To download the original data, go to # https://scholar.harvard.edu/files/dell/files/nationbuilding.pdf # The file we will use is there as `firstclose_post.dta&#39; df &lt;- as.data.frame(read.csv(&quot;vietnam_war.csv&quot;, stringsAsFactors = F)) lm() is the command, included in base R, that allows us to perform linear regression. To carry out a regression, we take a dataframe object including all of our dependent and independent variables. The first argument to lm() is a formula. This is the formula for our regression model. We specify it with the following syntax: dependent_variable ~ independent variable 1 + independent variable 2 +  + independent variable n. The variable names have to be the same as the names of the variables in the dataframe. Otherwise, lm() will not recognise the name of the variable. We do not have to pass the formula or variable names as strings - we just write them out in text, and lm() finds the variable with that name in our dataframe. lm() automatically includes an intercept. If we want to exclude this, write 0 as the first variable in our formula. The second argument to lm() is data. We need to pass the name of the dataframe containing our data. It is important to know what type of vector each of your variables is within the dataframe. If your variable is one of the numeric vector types, lm() will treat it as a continuous variable. If it is a factor, it will treat it as a categorical variable, with an ordering given by the ordering of the underlying factor. Thus, it will include a series of dummy variables or fixed effects for each level of the factor, omitting the one corresponding to the lowest level to avoid pure multicolinearity. This is basically it! There are other additional arguments that allow you to specify vectors of weights for weighted least squares, different fitting methods, and so on. # Lets run a few different linear regressions with lm # regressing mean airstrikes on whether the hamlet was above or below the algorithm # scoring threshold US planners used to assign planned airstrikes lm(fr_strikes_mean ~ below, data=df) ## ## Call: ## lm(formula = fr_strikes_mean ~ below, data = df) ## ## Coefficients: ## (Intercept) below ## 0.257591 0.009856 # lets drop the intercept lm(fr_strikes_mean ~ 0 + below, data=df) ## ## Call: ## lm(formula = fr_strikes_mean ~ 0 + below, data = df) ## ## Coefficients: ## below ## 0.2674 # regressing mean airstrikes on distance from the threshold with a second-order # polynomial trend # we will save this as an object m2 so we can take a look at it later df[[&quot;md_ab_square&quot;]] &lt;- df[[&quot;md_ab&quot;]]^2 m2 &lt;- lm(fr_strikes_mean ~ md_ab + md_ab_square, data=df) # now lets use factors to add year fixed effects m_time &lt;-lm(fr_strikes_mean ~ as.factor(yr) +md_ab + md_ab_square, data=df) As we can see, the output from the lm() command by itself it quite ugly. It is better to save it as a variable, and then we can look at it more nicely and do things with it. The first thing is to make it look a (little) nicer. We can do this using the base R summary function. Running summary(model_name) will output us a regression table containing OLS standard errors, p-values, R-squared, and the F statistic for joint significance of all coefficients. summary(m2) ## ## Call: ## lm(formula = fr_strikes_mean ~ md_ab + md_ab_square, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.3079 -0.2412 -0.1059 0.1609 1.0993 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.305918 0.002982 102.59 &lt; 2e-16 *** ## md_ab -0.303735 0.043391 -7.00 2.69e-12 *** ## md_ab_square -11.375115 0.345114 -32.96 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2946 on 12204 degrees of freedom ## Multiple R-squared: 0.08179, Adjusted R-squared: 0.08164 ## F-statistic: 543.5 on 2 and 12204 DF, p-value: &lt; 2.2e-16 # lets save it as an object so we can inspect it later too s2 &lt;- summary(m2) Both of these objects are structured as lists of lists. The whole object itself is a list, and each of the relevant outputs - e.g the coefficients - are a list within that list. We can eyeball what is in there by saving our summary object as an object, and clicking on the object name in our environment. This brings up the following. We can see here that our object is a list, and that it contains a set of other lists. Clicking on the lists with arrows on their left reveals the content of the list, and shows us the names attached to each entry. In general, this shows us what we have in our object and thus what we can slice out and present. The objects also store things like the variable names of each coefficient. The list structure and attached metadata means that we can slice the object using similar slicing techniques to the ones we learned earlier to extract what we want. This can be useful if, for example, we want to do some plotting, get a single test statistic, or use the coefficient values to predict out of sample. # lets get some results from our regressions # imagine for example that we want to plot the residuals to look for some # heteroskedasticity resids &lt;- s2[[&quot;residuals&quot;]] plot(df[[&quot;md_ab&quot;]], resids, col=&quot;red&quot;, xlab=&quot;Distance from threshold&quot;, ylab=&quot;Residual&quot;, main = &quot;Regression residuals against distance from score threshold&quot;) # we want to look at the 35th residual from this model for some reason s2[[&quot;residuals&quot;]][35] ## 35 ## 0.04256734 # or we just want to look at the coefficient on md_ab m2[[&quot;coefficients&quot;]][[&quot;md_ab&quot;]] ## [1] -0.303735 Most other regression routines in R either use a version of this function and output structure, or something deliberately very similar (e.g plm() for linear panel data models, pgmm() for panel data GMM models, dynlm() for time series). Thus, if you fit one of these models you get a lm() type object out that you can inspect, and then slice to get what you need, in a similar way. # An example of other lm-type objects in R - IV models # IV regression in R library(AER) ## Loading required package: car ## Loading required package: carData ## Loading required package: lmtest ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## Loading required package: sandwich ## Loading required package: survival ## Warning: package &#39;survival&#39; was built under R version 4.0.5 # lets do a fuzzy-rdd type IV regression to demonstrate # We look at the causal effect of airstrikes on local insurgent infiltration # using distance from the threshold as an instrument for airstrikes (and including # a quadratic trend) # syntax is second stage regression|dependent variables in first stage m_iv &lt;- ivreg(guer_squad ~fr_strikes_mean + md_ab_square|md_ab + md_ab_square, data=df) s_iv &lt;- summary(m_iv) # see how this package uses the base lm object to build up the more complex # econometric model, so that we can slice the resulting object in the same # way as before The lm object can also evaluate the model as strings. Thus, a neat trick we can use to fit large models or many similar models is to construct the model formula as a string using paste(), and then pass that string to the formula argument of lm(). # Imagine we want to include the variables in columns 4:50 of our dataframe into # our model as regressors # this syntax may look intimidating, so lets look at it piece by piece # As we learned when we looked at slicing earlier, df[,c(4:50)] will select # all rows (as the left hand side of the comma is empty) of columns 4:50 (as we # pass the vector of numbers 4:50 on the right hand side of the column) from our # dataframe df. # names() returns the column name of a given dataframe object. Thus, names(df[,c(4:50)]) # returns a vector of names of all of the columns in our slice as strings # paste() takes in vectors of strings, or multiple strings manually, and returns # a single string. If we specify the &#39;collapse&#39; argument, it takes any vector # of strings and returns a single string made up of each of the elements of the # vector pasted together, and separated by the thing we pass to &#39;collapse&#39;. In # this case, it takes all of our names(df[,c(4:50)]) and puts them in a single # string, each separated by a &#39;+&#39; sign dep_vars &lt;- paste(names(df[,c(4:50)]), collapse=&quot;+&quot;) # next we need to add our independent variable. We can do this with &#39;paste&#39; again. # If we specify &#39;sep&#39; and pass strings as individual arguments (instead of a vector), # paste takes each string we pass and puts them together in # one string separated by the symbol we pass to &#39;sep&#39;. formula_string &lt;- paste(&quot;fr_strikes_mean&quot;, dep_vars, sep=&quot;~&quot;) # now we have our formula, we can run the regression model m_string &lt;- lm(formula_string, data=df) summary(m_string) ## ## Call: ## lm(formula = formula_string, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.92720 -0.11849 -0.02094 0.08878 0.92147 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.460e-02 2.955e-02 1.509 0.131249 ## fr_forces_mean 1.651e-01 7.819e-03 21.110 &lt; 2e-16 *** ## pop_g -1.426e-01 1.472e-02 -9.687 &lt; 2e-16 *** ## naval_attack 6.203e-02 8.221e-02 0.755 0.450564 ## sh_pf_presence -1.430e-02 6.744e-03 -2.120 0.033994 * ## sh_rf_presence -3.333e-02 1.163e-02 -2.866 0.004162 ** ## fr_init -8.907e-02 5.815e-02 -1.532 0.125634 ## fw_init -1.415e+00 4.552e-01 -3.108 0.001890 ** ## en_d 3.473e-05 1.881e-04 0.185 0.853526 ## fw_d -3.587e-02 1.168e-02 -3.073 0.002128 ** ## fr_d 7.137e-04 1.154e-03 0.618 0.536382 ## fr_opday_dummy 1.011e-01 5.724e-02 1.765 0.077516 . ## fw_opday_dummy 1.306e+00 4.589e-01 2.846 0.004434 ** ## vc_infr_vilg -7.916e-03 9.467e-03 -0.836 0.403053 ## part_vc_cont 8.758e-03 3.903e-02 0.224 0.822450 ## guer_squad 8.530e-02 7.902e-03 10.795 &lt; 2e-16 *** ## mainforce_squad 1.286e-01 8.226e-03 15.629 &lt; 2e-16 *** ## en_base 3.031e-03 8.903e-03 0.340 0.733517 ## entax_vilg 5.570e-02 8.042e-03 6.926 4.59e-12 *** ## phh_psdf -1.480e-02 1.160e-02 -1.276 0.201842 ## psdf_dummy -4.234e-02 1.197e-02 -3.538 0.000405 *** ## chief_visit 5.704e-02 1.352e-02 4.219 2.48e-05 *** ## village_comm -4.277e-02 9.169e-03 -4.665 3.13e-06 *** ## gvn_taxes -4.322e-02 7.493e-03 -5.767 8.30e-09 *** ## rdc_active -5.534e-02 6.824e-03 -8.110 5.68e-16 *** ## civic_org_part -6.545e-02 1.134e-02 -5.772 8.08e-09 *** ## vilg_council_meet 3.150e-02 6.653e-03 4.736 2.22e-06 *** ## youth_act 3.903e-03 8.058e-03 0.484 0.628098 ## p_own_vehic -5.310e-02 1.635e-02 -3.248 0.001166 ** ## nonrice_food 1.841e-02 9.894e-03 1.861 0.062810 . ## manuf_avail -1.264e-02 9.302e-03 -1.359 0.174230 ## surplus_goods 3.832e-02 6.667e-03 5.748 9.28e-09 *** ## econ_train 5.062e-02 1.235e-02 4.100 4.17e-05 *** ## self_dev_part -2.065e-02 1.055e-02 -1.957 0.050347 . ## selfdev_vilg -6.659e-02 1.504e-02 -4.428 9.60e-06 *** ## pworks_under_constr 2.016e-02 7.014e-03 2.875 0.004049 ** ## prim_access 3.258e-02 1.087e-02 2.996 0.002741 ** ## sec_school_vilg 3.092e-02 6.321e-03 4.891 1.02e-06 *** ## p_require_assist 4.766e-02 2.051e-02 2.323 0.020192 * ## nofarm_sec 4.624e-02 7.037e-03 6.571 5.24e-11 *** ## urban -4.235e-02 1.035e-02 -4.091 4.33e-05 *** ## buddhist 1.146e-02 5.236e-03 2.189 0.028649 * ## farming -1.872e-02 7.110e-03 -2.633 0.008482 ** ## all_atk 2.945e-01 1.794e-02 16.418 &lt; 2e-16 *** ## en_pres 1.711e-01 1.821e-02 9.399 &lt; 2e-16 *** ## en_prop -1.060e-01 2.338e-02 -4.533 5.89e-06 *** ## admin_p1 1.017e-01 2.800e-02 3.634 0.000281 *** ## health_p1 -4.880e-02 8.736e-03 -5.586 2.39e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2027 on 9761 degrees of freedom ## (2398 observations deleted due to missingness) ## Multiple R-squared: 0.5178, Adjusted R-squared: 0.5155 ## F-statistic: 223 on 47 and 9761 DF, p-value: &lt; 2.2e-16 4.1.2 Computing covariance matrices with sandwich lm only computes standard errors based on the standard OLS variance-covariance matrix. We often want to do something different, for example computing Newey-West standard errors. The package sandwich allows us to easily compute these kind of sandwich covariance matrices. There are multiple different sandwich commands, corresponding to different types of covariance matrix. Generally, we need to pass these object a fitted model object like our lm() model. Next, we can then specify whether we need to do a finite sample adjustment (if we want to make our output consistent with Stata, which often does this automatically), if we want to pass a weights matrix, and so on. If we want to specify the outsides and insides of the covariance matrix structure manually, we can do so using bread() and meat(). # Lets compute some different types of covariance # matrices with sandwich library(sandwich) # heteroskedasticity and autocorrelation robust covariance matrix m2_HAC_cov_mat &lt;- vcovHAC(m2) # Panel-corrected covariance matrix m2_pan_cov_mat &lt;- vcovPC(m2, cluster=df[[&quot;usid&quot;]]) # now if we want to get the standard errors back, we need to take the square root # of the diagonal of the matrix m2_HAC_ses &lt;- sqrt(diag(m2_HAC_cov_mat)) Once we have these, we might want to look quickly at which variables are significant. We can do this by using the coeftest() function from the package lmtest. The package lmtest contains functions for lots of specification tests for different linear models (to name three examples: likelihood ratio tests, reset tests, and tests for Granger causality). coeftest() is a counterpart to the lm() object. We pass it a lm() object and a method for computing a sandwich covariance matrix. The function then prints out the object summary with the appropriate significance levels and p-values from our covariance matrix. library(lmtest) coeftest(m2, vcov=vcovHAC) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.3059175 0.0098534 31.0469 &lt; 2.2e-16 *** ## md_ab -0.3037350 0.0645894 -4.7026 2.597e-06 *** ## md_ab_square -11.3751147 0.6366604 -17.8668 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.1.3 Useful packages for econometrics A (not exhaustive) list of some useful packages for econometrics with R is: - base - the inbuilt set of functions that come with R; - ctest for classical statistical tests; - MASS, lmtest, and AER for cross-sectional models with OLS; - nls for non-linear regression - MatchIt, and RCT for causal inference; - plm for panel data analysis; - ts, timeseries, vars, forecast, dynlm, lubridate, zoo for time series analysis; and - igraph for network analysis. 4.2 Matrix algebra If we want to program our own estimators, we should use matrices. This means we have to know how to do matrix operations in R. Here, we go through some of the basic matrix operations in R. We use them to program a least-squares estimator directly from one of the groups of the data we created above. # Common matrix operations # first way we can create a matrix is by taking the entries of a dataframe, # and converting them to a matrix with as.matrix X_mat &lt;- as.matrix(df[[&quot;md_ab&quot;]], df[[&quot;md_ab_square&quot;]]) y_mat &lt;- as.matrix(df[[&quot;fr_strikes_mean&quot;]]) # create a matrix using matrix, and specifying the rows (nrow) and columns (ncol) # Let i,j denote the entry on the ith row and jth column # we pass a vector of numbers to get the entries # the vector should have the form # c(1,1; 1,2; 1,3; ..... 2,1; 2,2; ..... n,n) A_mat &lt;- matrix(c(1,4,1,3,2,2,5,4,3), nrow=3, ncol=3) B_mat &lt;- matrix(c(1,1,1,2,2,2,3,3,3), nrow=3, ncol=3) # elementwise multiplication elementwise_mat &lt;- A_mat * B_mat # matrix multiplication mult_mat &lt;- A_mat %*% B_mat # outer product out_mat &lt;- A_mat %o% B_mat # transpose A_t &lt;- t(A_mat) # cross product i.e A&#39;B - two ways of doing it cross_mat_1 &lt;- t(A_mat)%*%B_mat cross_mat_2 &lt;- crossprod(A_mat, B_mat) # Matrix of squares A&#39;A - two ways of doing it sq_mat_1 &lt;- t(A_mat)%*%A_mat sq_mat_2 &lt;- crossprod(A_mat, A_mat) # creating diagonal matrices # wrapping a matrix in diag returns the diagonal matrix from the principal # diagonal of the matrix A_diag &lt;- diag(A_mat) # passing a vector to diag gives a matrix with that on the principal diagonal # If we pass only a scalar a, it returns an a by a identity matrix instead (no idea why # this is just how they set it up) A_diag &lt;- diag(c(1,2,3)) I_mat&lt;- diag(3) # Inverses - solve() inverts the matrix, ginv() computes the Moore-Penrose # generalised pseudo-inverse (need the MASS package for this) A_inv &lt;- solve(A_mat) library(MASS) A_inv_mp &lt;- ginv(A_mat) # Eigenvalues and eigenvectors - eigen() computes eigenvectors [[&quot;vectors&quot;]] and # corresponding eigenvalues [[&quot;values&quot;]] A_eigens &lt;- eigen(A_mat) A_eigens[[&quot;vectors&quot;]] ## [,1] [,2] [,3] ## [1,] -0.5994073 -0.4695498 -0.01833495 ## [2,] -0.6888071 0.8410090 -0.85517793 ## [3,] -0.4077447 -0.2687506 0.51801017 A_eigens[[&quot;values&quot;]] ## [1] 7.8486741 -1.5114986 -0.3371754 # singular value decomposition y &lt;- svd(A_mat) # finally, lets compute one of our least squares estimators beta_vec &lt;- solve((t(X_mat) %*% X_mat)) %*% t(X_mat) %*% y_mat # now computing variance-covariance matrix resid_vec &lt;- y_mat - X_mat %*% beta_vec vcov &lt;- solve((t(X_mat) %*% X_mat)) %*% t(X_mat) %*% resid_vec %*% t(resid_vec) %*% X_mat %*% solve((t(X_mat) %*% X_mat)) "],["workflow.html", "Chapter 5 Workflow 5.1 Regression tables with Stargazer 5.2 General tables with XTable 5.3 Writing documents with RMarkdown 5.4 Sharing code using RStudio projects", " Chapter 5 Workflow We have computed our models. Now we need to write up our results and put them in nice tables. If you want to do this from scratch, it can be really annoying. If you do not believe me, try writing the regression tables out by hand in TeX . Furthermore, you have to do it every time you change your model, change variables, or fix a bug. You might also be working in a group, where you want to share your code with each other, edit it collaboratively and so on. Luckily, we can automate this all in R! Below, we first go through how to automatically output journal-quality LaTeX or HTML tables in R every time you run your code, and to share code across teams. This gives us two ways of nicely automating our workflow. Firstly, if you want to write your document in a LaTeX, you can install a local LaTeX editor on your machine (I personally prefer TeXMaker https://www.xm1math.net/texmaker/) and output TeX tables to the same location you store your TeX file. Then, whenever you run your code, it will also update the tables in your LaTeX document. Otherwise, we can actually write the entire document in R as something called an RMarkdown file (like these notes). Then, as we have here, we can write our report and code in the same document and thus see our tables and so on directly in the document. We first cover how to automate creating tables using Stargazer. We then demonstrate how to do it for a general dataframe using XTable. We do not have the space to do RMarkdown justice here, so take a look at the introductory files on their website https://rmarkdown.rstudio.com/lesson-1.html. 5.1 Regression tables with Stargazer When we have our lm-type output object, and our standard errors, we want to put them in a nice regression table with the appropriate specification tests that we can put in our report or paper. Typically, this will be a LaTeX table. The most common way to automat this is the stargazer package we saw briefly earlier. It is very flexible, giving us lots of choice what to include or exclude, and supports both Tex and HTML tables. The basic command in stargazer is stargazer(). This is the command to create the table. As the first argument, we pass a single regression model or a list of regression models. If we pass a list of regression models, it will automatically put each model in its own column in the table. library(stargazer) ## ## Please cite as: ## Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. ## R package version 5.2.2. https://CRAN.R-project.org/package=stargazer # To download the original data, go to # https://scholar.harvard.edu/files/dell/files/nationbuilding.pdf # The file we will use is there as `firstclose_post.dta&#39; library(sandwich) library(stargazer) #setwd(&quot;C://Users//kiera//OneDrive//Documents//Tinbergen MPhil//Econometrics_1_TA//intro_R_TI&quot;) df &lt;- as.data.frame(read.csv(&quot;vietnam_war.csv&quot;, stringsAsFactors = F)) df[[&quot;md_ab_square&quot;]] &lt;- df[[&quot;md_ab&quot;]]^2 m2 &lt;- lm(fr_strikes_mean ~ md_ab + md_ab_square, data=df) stargazer(m2, type=&quot;html&quot;) Dependent variable: fr_strikes_mean md_ab -0.304*** (0.043) md_ab_square -11.375*** (0.345) Constant 0.306*** (0.003) Observations 12,207 R2 0.082 Adjusted R2 0.082 Residual Std. Error 0.295 (df = 12204) F Statistic 543.513*** (df = 2; 12204) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 By default, stargazer returns the code for a LaTeX table object with the basic standard errors. These look like the tables you see in journals. Here, we use html output to integrate it into the document. There are many, many different arguments we can specify to change the style of the table, titles and variable names, what we include or omit, and different standard errors. Go and look at the package vignette at https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf for a list of all of the arguments and commands. In the code section below, we will demonstrate some of the most common and useful ones. # Lets use the m2 and m_time models from before to demonstrate how Stargazer # works # now lets use factors to add year fixed effects m_time &lt;-lm(fr_strikes_mean ~ as.factor(yr) +md_ab + md_ab_square, data=df) m2_HAC_ses &lt;- sqrt(diag(vcovHAC(m2))) # lets add some labels to start with and report coefficients with two digits # these need to be written in the appropriate TeX # write all backslashes &#39;\\&#39; as &#39;\\\\&#39; instead stargazer(m2, title = &quot;Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets&quot;, dep.var.caption = &quot;Dependent variable: Mean foreign airstrikes&quot;, intercept.bottom=T, digits = 2, covariate.labels = c(&quot;$\\\\text{Distance}^{2}$&quot;, &quot;$\\\\text{Distance}$^{2}&quot;), type=&quot;html&quot;) Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets Dependent variable: Mean foreign airstrikes fr_strikes_mean Distance2 -0.30*** (0.04) Distance2 -11.38*** (0.35) Constant 0.31*** (0.003) Observations 12,207 R2 0.08 Adjusted R2 0.08 Residual Std. Error 0.29 (df = 12204) F Statistic 543.51*** (df = 2; 12204) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 # now lets manually specify the cutoffs for significance stars stargazer(m2, title = &quot;Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets&quot;, dep.var.caption = &quot;Dependent variable: Mean foreign airstrikes&quot;, intercept.bottom=T, digits = 2, covariate.labels = c(&quot;$\\\\text{Distance}^{2}$&quot;, &quot;$\\\\text{Distance}$^{2}&quot;), star.cutoffs = c(0.05,0.01,0.001), type=&quot;html&quot;) Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets Dependent variable: Mean foreign airstrikes fr_strikes_mean Distance2 -0.30*** (0.04) Distance2 -11.38*** (0.35) Constant 0.31*** (0.003) Observations 12,207 R2 0.08 Adjusted R2 0.08 Residual Std. Error 0.29 (df = 12204) F Statistic 543.51*** (df = 2; 12204) Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 # now we can add some different standard errors - our HAC standard errors from # before stargazer(m2, title = &quot;Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets&quot;, dep.var.caption = &quot;Dependent variable: Mean foreign airstrikes&quot;, intercept.bottom=T, digits = 2, covariate.labels = c(&quot;$\\\\text{Distance}^{2}$&quot;, &quot;$\\\\text{Distance}$^{2}&quot;), star.cutoffs = c(0.05,0.01,0.001), se.list = m2_HAC_ses, type=&quot;html&quot;) Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets Dependent variable: Mean foreign airstrikes fr_strikes_mean Distance2 -0.30*** (0.04) Distance2 -11.38*** (0.35) Constant 0.31*** (0.003) Observations 12,207 R2 0.08 Adjusted R2 0.08 Residual Std. Error 0.29 (df = 12204) F Statistic 543.51*** (df = 2; 12204) Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets Distance2 Distance2 md_ab_square 0.01 0.06 0.64 # now lets add another regression - including the time dummies - and choose # not to display the dummies as they look messy m_list &lt;- as.list(m2, m_time) # computing some HAC standard errors for these models m_time_HAC_ses &lt;- sqrt(diag(vcovHAC(m_time))) ses &lt;- as.list(m2_HAC_ses, m_time_HAC_ses) stargazer(m_list, title = &quot;Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets&quot;, dep.var.caption = &quot;Dependent variable: Mean foreign airstrikes&quot;, column.labels = c(&quot;No time trend&quot;, &quot;Time trend&quot;), intercept.bottom=T, digits = 2, covariate.labels = c(&quot;$\\\\text{Distance}^{2}$&quot;, &quot;$\\\\text{Distance}$^{2}&quot;), star.cutoffs = c(0.05,0.01,0.001), se.list = ses, omit=&quot;yr&quot;, type=&quot;html&quot;) Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets Dependent variable: Mean foreign airstrikes fr_strikes_mean No time trend Distance2 -0.30*** (0.04) Distance2 -11.38*** (0.35) Constant 0.31*** (0.003) Observations 12,207 R2 0.08 Adjusted R2 0.08 Residual Std. Error 0.29 (df = 12204) F Statistic 543.51*** (df = 2; 12204) Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets 0.01 Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets 0.06 Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets 0.64 # finally, lets output it as a tex file called &#39;reg_table.tex&#39; stargazer(m_list, out=&quot;reg_table.tex&quot;, title = &quot;Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets&quot;, dep.var.caption = &quot;Dependent variable: Mean foreign airstrikes&quot;, column.labels = c(&quot;No time trend&quot;, &quot;Time trend&quot;), intercept.bottom=T, digits = 2, covariate.labels = c(&quot;$\\\\text{Distance}^{2}$&quot;, &quot;$\\\\text{Distance}$^{2}&quot;), star.cutoffs = c(0.05,0.01,0.001), se.list = ses, omit=&quot;yr&quot;, type=&quot;html&quot;) Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets Dependent variable: Mean foreign airstrikes fr_strikes_mean No time trend Distance2 -0.30*** (0.04) Distance2 -11.38*** (0.35) Constant 0.31*** (0.003) Observations 12,207 R2 0.08 Adjusted R2 0.08 Residual Std. Error 0.29 (df = 12204) F Statistic 543.51*** (df = 2; 12204) Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets 0.01 Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets 0.06 Effect of closeness to threshold on airstrike intensity in South Vietnamese hamlets 0.64 5.2 General tables with XTable We might want to print something other than a regression table though as a LaTeX table. In general, we can compute lots of different things we might want to show our readers. If we can compute them, we can put them in a data frame. We just need a way to turn the dataframe into a nice LaTeX or HTML table. The xtable package allows us to do so. The xtable package is built around the xtable() function. This function creates a nice table in the format we want. Then, we need to print() it. This allows us to save it as a file in the type of our choice. Again, there are lots of arguments to both functions. We do not go through them all here - instead read the documentation for the functions if you are interested. We will demonstrate the functionality by creating a balancing table for some of our data using the RCT package, # Lets construct a balancing table for hamlets occupied by the US Army vs USMC library(data.table) library(RCT) ## Warning: package &#39;RCT&#39; was built under R version 4.0.5 library(xtable) df_mar &lt;- as.data.frame(read.csv(&quot;marines_hamlet.csv&quot;)) # creating a balacing table using the balance_table function from RCT # first argument is the data frame containing all our variables we want # to check, and the second argument is our treatment variable # lets store it as a dataframe, and rename the columns so they look a bit # nicer sum_tbl &lt;- as.data.frame(balance_table(df_mar[,c(4:7, 9:12, 42)], &quot;treat&quot;)) names(sum_tbl) &lt;- c(&quot;Variable&quot;, &quot;Mean - USMC hamlets&quot;, &quot;Mean - US Army hamlets&quot;, &quot;P-value for difference in means&quot;) # now lets store our balancing table in a nice format using the xtable # function xtab &lt;- xtable(sum_tbl, caption=&quot;Balancing table&quot;, digits=2) # finally, lets store it as a TeX file using `print` # First argument has to be the xtable object # file argument gives the file name # we can change the location in our computer by changing the # working directory with setwd(file_path_as_string) print(xtab, include.rownames=FALSE, file=&quot;sum_tbl.tex&quot;) 5.3 Writing documents with RMarkdown Markdown is the R way of creating a notebook - chunks of code set between chunks of text, compiled in Tex or HTML form. It is similar to a Jupyter Notebook for Python. Markdown is a very nice way to present code and results together for an assignment or technical document. For more detail, see R Markdown - the Definitive Guide https://bookdown.org/yihui/rmarkdown/ . Start by installing the package rmarkdown - run install.packages('rmarkdown') in the console. To open a markdown document in RStudio, go to file. Click on file. You will see a drop-down menu. You can select either R Notebook or R Markdown. R Notebook, is a newer version of the Markdown document, so generally use this. Then you will see a screen like the following with a white background, punctuated by grey chunks. The white space is where you write text. Add headers by putting #, ##, or ### ahead of sections of text. The grey chunks are where you can write your code. Add new chunks of code using the insert button on the top right of the screen. Run and test individual chunks by pressing the play button on the top right of the chunk. When you are ready to compile the whole document, knit it by pressing the knit button on the top left. knit the file, it runs the code, and presents the output together in either a pdf document with Tex (if you have Tex installed on your machine), or a HTML document. If you output something in your code, like a graph, R will display it below the code. 5.4 Sharing code using RStudio projects We often use multiple files for a single piece of work. There might be files containing data, scripts doing different things, or README files to explain what everything does. To work effectively, we need to keep track of where everything is. RStudio has a built in way of doing this using something called a project. Put simply, if you create your scripts inside a project, it will put them all in the same place so that we can run them all together effectively. To start a project, click on file and select a new project. You can then create a new project in a new directory, or an existing directory. Once you create the project, you can save your files or open new scripts within that project. Instead of directly opening a file, you can now open the project which contains all the correct scripts and the correct directory. If you set up RStudio accounts, you can share your projects with other people by going to file and share project. Then you can each edit the same files at the same time collaboratively without overwriting each others work. If you want to, you can set this up as a GitHub repository. Github is beyond the scope of this guide, but you should learn how to do this. "],["advanced-programming-techniques.html", "Chapter 6 Advanced programming techniques 6.1 Advanced iteration and Monte-Carlo simulation 6.2 Optimisation 6.3 Profiling", " Chapter 6 Advanced programming techniques By this point, we have covered the basics of programming in R. Now we have done this, we will present a little more advanced material that can be useful and make your code better. These are based around programming your own estimators. First, we will show you how to simulate some data to use with Monte-Carlo simulation. We will use the vectorising tricks we learnt in the first section. Then, we will use this data to demonstrate optimisation in R. We will do this by programming a maximum-likelihood estimator. 6.1 Advanced iteration and Monte-Carlo simulation First, we will generate some of our own data to use by doing some Monte-Carlo simulations. This is a useful thing to be able to do by itself. Furthermore, we will use it to reinforce some of the principles of R we learned earlier, and introduce some more cool advanced features you might want to use. Here, we do not simulate our data in the most efficient way that we could. For some more advanced ways of doing so, see this great article by Grant McDermott https://grantmcdermott.com/efficient-simulations-in-r/#fn:1 We are generating some random numbers. So we should start by setting a seed for our random number generator. Seeding sets the starting value for the random number generators in your program. As random number generators are deterministic, setting a seed ensures that you get the same random numbers each time that you run your code. This helps with debugging, and ensures others can reproduce our code. We can do this in R with set.seed(). # lets set a seed for reproducibility rm(list=ls()) set.seed(854) Next, we need to generate some random numbers. Base R has a lot of nice inbuilt commands for sampling from different distributions. The synatx is generally as follows. In the first argument, you specify the number of draws you want to make. In the next arguments, you specify the parameters of the distribution - e.g the mean and standard deviation for a normal distribution. R then outputs a numeric vector composed of draws from the distribution. # Some examples of drawing random numbers in R # the command &#39;runif&#39; gives draws from a uniform [0,1] distribution if you do # not specify a support unif_1 &lt;- runif(35) # but you can specify a support, using the &#39;min&#39; and &#39;max&#39; parameters unif_2 &lt;- runif(35, min= 20, max= 40) # the command &#39;rnorm&#39; gives draws from a normal distribution with parameters &#39;mean&#39;, &#39;sd&#39; # lets do 20 draws from a standard normal distribution norm_1 &lt;- rnorm(20, mean=0, sd=1) # the command rchisq gives draws from a chi-square distribution with degrees of # freedom &#39;df&#39; chi_sq_1 &lt;- rchisq(34, df=100) # finally, rpois gives draws from a poisson distribution with rate parameter # &#39;lambda&#39; pois_1 &lt;- rpois(22, 6) In general, the syntax for random number draws in R is composed of a root name, that we precede with a prefix from (p,q,d,r). The root name is a shorthand for the name of the distribution. P gives us the cumulative probability for a given value from the c.d.f, q gives us the inverse of the c.d.f (i.e quantiles), d gives us the probability mass from the denisty function, and r draws random numbers from the set distribution. For a list of the roots for common variables in R, see https://www.stat.umn.edu/geyer/old/5101/rlook.html. Now we can generate some random numbers, we need to efficiently generate lots of them. For illustration, lets simulate a linear regression model in three variables. Each of the variables is itself normally distributed, and we have a normally distributed error term. We will also only do small simulations of 1000 data points for ease. Our first instinct when doing something like this might be to just use a for or while loop. We might also think that to store our values, we should initialise an empty vector and then fill it with values. # How not to do simulation in R vec_x1 &lt;- c() vec_x2 &lt;- c() vec_x3 &lt;- c() i &lt;- 0 while (i &lt;100){ vec_x1 &lt;- c(vec_x1, rnorm(1, 2, 1)) i &lt;- i + 1 } j &lt;- 0 while (j &lt;100){ vec_x2 &lt;- c(vec_x2, rnorm(1, 2, 1)) j &lt;- j + 1 } k &lt;- 0 while (k &lt;100){ vec_x3 &lt;- c(vec_x3, rnorm(1, 2, 1)) k &lt;- k + 1 } This is not a good way to go about this for two reasons. Firstly, we do not want to do our simulation using loops because R is a vectorised language. Practically, being vectorised means that operations involving vectors are much much faster than loops. R is different in this way to other languages like Python and C++, which are object-oriented instead. In object-oriented languages, loops are very efficient and can even sometimes be faster than operations on vectors. As the amount of data we want to simulate here is small, this does not matter so much. It will begin to matter more if we try to simulate a lot more data, however. Secondly, we do not want to store more data by expanding a vector. The way R stores objects in memory makes this very memory intensive and inefficient. It is much better to create an object of right length, and then replace the values we need to use. We can create a vector of a given length using the rep function. The first argument of this is some value. A good choice to use is NA, so we can work out if our filling steps later fail. The second argument is the length of the vector you want to produce. Lets do the second first. We now reproduce one of the simulations above, but instead of growing a vector we create a vector of NAs, and then fill it up. # Some more efficient, vectorised, simulation code # creating empty vectors with rep vec_x1 &lt;- rep(NA, 100) i &lt;- 0 while (i &lt;100){ # note we have to use i+1 here as we start at 0, but R starts counting lengths # at 1 vec_x1[i+1] &lt;- rnorm(1, 2, 1) i &lt;- i + 1 } Next, lets replace the loop. We already know that we can draw vectors of random variables directly. We might not be able to do this if our use case is a bit more complicated though. Thus, we should also look at applying a function to generate the data. We will do this by generating 100 sets of our observations, storing these as a dataframe, and then applying a function to fit regression models to these observations. #-------------------------------------------------------------------------------- # Some more efficient Monte-Carlo # Say we want to find the distribution of the estimates of \\beta_{1} in a # linear regression model of the form y_{i}= \\alpha + \\sum_{j}\\beta_{j}x_{i}^{j} + e_{i} # up to the third order #-------------------------------------------------------------------------------- # Parameters ------------------------------------------------------------------- # Number of simulations mc &lt;- 100 # Number o data points within each simulation length_within_sim &lt;- 100 # &#39;True parameters&#39; of the regression model alpha = 2.2 beta_1 = 0.5 beta_2 = 0.3 beta_3 = 0.2 # Functions ------------------------------------------------------------------- # here we are going to group our dataframe by a group indicator, and # then fit the linear regression to subsets of the data by the group # indicator fit_reg &lt;- function(group, group_col, df, reg_formula){ # note that it is acutally more efficient here to use the lm.fit # method - if you would like to do something more advanced take # a look at that with ?lm.fit mod &lt;- lm(reg_formula, data=mc_df[mc_df[[group_col]]==group,]) return(mod[[&quot;coefficients&quot;]][2]) } # Running code ----------------------------------------------------------------- # Generating our variables int &lt;- rep(mc*length_within_sim, alpha) vec_x1 &lt;- rnorm(mc*length_within_sim , 2, 1) vec_err &lt;- rnorm(mc*length_within_sim, 0,1) # Here we are fitting a cubic function. Thus, we simulate the new # variables by operating on the old ones. This is more efficient # (Thanks to Katya Ugulava for pointing this out to me) vec_x2 &lt;- vec_x1^2 vec_x3 &lt;- vec_x1^3 # now creating a group indicator for each of our simulations using the each # command in rep group &lt;- rep(c(1:100),each=100) # putting these ina dataframe to then fit the regression mc_df &lt;- as.data.frame(cbind(group, int, vec_x1, vec_x2, vec_x3)) names(mc_df) &lt;- c(&quot;group&quot;,&quot;int&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;) # lets just change the rownames now for ease of use # notice we are using fast vectorised multiplication here! mc_df[[&quot;y&quot;]] &lt;- mc_df[[&quot;int&quot;]] + beta_1 * mc_df[[&quot;x1&quot;]] + beta_2 * mc_df[[&quot;x2&quot;]] + beta_3 * mc_df[[&quot;x3&quot;]] + vec_err # finally, lets apply our fitting function above to the dataframe # to get a set of estimates of \\beta_{1} beta_vec&lt;- unlist(lapply(1:100, FUN=fit_reg, group_col=&quot;group&quot;, df=mc_df, reg_formula = &quot;y~x1+x2+x3&quot;)) hist(beta_vec, breaks=25, col=&quot;red&quot;, main=&quot;Distribution of the estimator for beta_{1}&quot;) If we want to do this more times, we should write the whole Monte-Carlo operation as a function where we pass the parameter values as an argument. We can still make this more efficient though, by replacing our dataframe with an object called a data.table or a tibble, and then doing something called nesting the operation within the table. Here we cover how to do this with a data.table - for tibbles see https://r4ds.had.co.nz/many-models.html . A data.table is effectively an improved version of the base R data.frame. It comes in its own package with the same name. data.tables keep the same functionality as data,frames, but add the ability to perform operations on the rows and columns as part of the object. We will demonstrate these functionalities while performing our simulations. The most important of these is the by argument. by is an inbuilt argument to group data by the values in a column within the data.table. # Demonstrating some data table functionalities # before performing our Monte-Carlo library(data.table) # lets generate some data and use it # one random variable and two different # groups col_1 &lt;- rnorm(10000, 0,1) col_2 &lt;- rep(c(&quot;A&quot;, &quot;B&quot;), each=5000) # the command as.data.table allows us to convert an object into a data.table # if we already have a dataframe or list object, we can use DT() instead dt &lt;- as.data.table(cbind(col_1, col_2)) names(dt) &lt;- c(&quot;r_v&quot;, &quot;grp&quot;) # this code ensures our r_v column is a numeric variable dt[,r_v :=as.numeric(r_v)] # In addition to selecting columns by passing strings, in data.table we can now # just select columns based on names # data.table also recognises automatically which of our conditions are rows and # column names, so we do not have to include the data.table name to carry out # operations as in a data.frame dt[r_v &lt; -1 &amp; grp ==&quot;A&quot;] ## r_v grp ## 1: -1.703836 A ## 2: -1.674758 A ## 3: -1.641446 A ## 4: -1.029205 A ## 5: -1.831224 A ## --- ## 766: -1.729828 A ## 767: -1.713903 A ## 768: -1.821262 A ## 769: -1.930295 A ## 770: -1.403510 A # One inbuilt functionality is ordering! # ordering lexically by group, and then by the value of the random variable dt &lt;- dt[order(r_v)] # Some useful inbuilt functions # .N counts the number of observations with a preceeding condition dt[r_v&lt;-1 &amp; grp==&quot;A&quot;, .N ] ## [1] 5000 # by allows us to start doing some aggregation # lets compute the number of observations in each group dt[,.N, by=grp] ## grp N ## 1: A 5000 ## 2: B 5000 # we have to enclose custom functions in a bracket, and precede them with # a . to apply them to all observations that satisfy any condition in # the preceding argument dt[,.(mean(r_v)), by=grp] ## grp V1 ## 1: A 0.009828461 ## 2: B -0.012719875 dt[r_v &lt; -1,.(mean(r_v)), by=grp] ## grp V1 ## 1: A -1.521316 ## 2: B -1.551410 # we can chain these operations on the rows by adding [] containing a new # operation after the end of our code above dt[r_v &lt; -1,.(mean(r_v)), by=grp][order(grp)] ## grp V1 ## 1: A -1.521316 ## 2: B -1.551410 Above, we only provide a brief overview of what data.tables can do. Also read https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html . Now lets use data.table to nest our Monte-Carlo routine. The idea of nesting is the following. Typically, when we create an object like a data.frame or data.table, we think of each entry as being a single thing like a number or word. But this does not have to be the case. We can actually fill the entries in these objects with other objects, like lists or data.tables. data.table provides us with a set of inbuilt iteration operations that are more efficient than just using the base apply over subsets of a given data.frame or data.table. Thus, by storing subsets of our data as observations in our data.table we can exploit the inbuilt iteration operations to do any iteration we want in a more efficient way. If that sounds abstract, lets take a look at a concrete example by making our Monte-Carlo code a bit more efficient. We will use the .SD inbuilt function in data.table to create a data.table containing just nested subsets of our data by the value of a variable (in our case, our group variable we make during the simulation). #-------------------------------------------------------------------------------- # Some even more efficient Monte-Carlo using nested operations in data.table # Say we want to find the distribution of the estimates of \\beta_{1} in a # linear regression model of the form y_{i}= \\alpha + \\sum_{j}\\beta_{j}x_{i}^{j} + e_{i} # up to the third order #-------------------------------------------------------------------------------- # Parameters ------------------------------------------------------------------- # Number of simulations mc &lt;- 100 # Number o data points within each simulation length_within_sim &lt;- 100 # &#39;True parameters&#39; of the regression model alpha = 2.2 beta_1 = 0.5 beta_2 = 0.3 beta_3 = 0.2 # Functions ------------------------------------------------------------------- # here we are going to group our dataframe by a group indicator, and # then fit the linear regression to subsets of the data by the group # indicator fit_reg &lt;- function(group, group_col, df, reg_formula){ # note that it is acutally more efficient here to use the lm.fit # method - if you would like to do something more advanced take # a look at that with ?lm.fit mod &lt;- lm(reg_formula, data=mc_df[mc_df[[group_col]]==group,]) return(mod[[&quot;coefficients&quot;]][2]) } # Running the code ------------------------------------------------------------- # Generating our variables int &lt;- rep(mc*length_within_sim, alpha) vec_x1 &lt;- rnorm(mc*length_within_sim , 2, 1) vec_err &lt;- rnorm(mc*length_within_sim, 0,1) # Here we are fitting a cubic function. Thus, we simulate the new # variables by operating on the old ones. This is more efficient # (Thanks to Katya Ugulava for pointing this out to me) vec_x2 &lt;- vec_x1^2 vec_x3 &lt;- vec_x1^3 # now creating a group indicator for each of our simulations using the each # command in rep group &lt;- rep(c(1:100),each=100) # putting these in a dataframe to then fit the regression mc_df &lt;- as.data.table(cbind(group, int, vec_x1, vec_x2, vec_x3)) names(mc_df) &lt;- c(&quot;group&quot;,&quot;int&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;) # lets just change the rownames now for ease of use # notice we are using fast vectorised multiplication here! mc_df[[&quot;y&quot;]] &lt;- mc_df[[&quot;int&quot;]] + beta_1 * mc_df[[&quot;x1&quot;]] + beta_2 * mc_df[[&quot;x2&quot;]] + beta_3 * mc_df[[&quot;x3&quot;]] + vec_err # now lets define a data.table containing a new column &#39;data&#39; which is our # data subsetted by the value of the group variable mc_dat = mc_df[, list(data=list(.SD)), by=group] # now we can use the inbuilt data.table iterators to fit all of our regression # models # what this does here is define a new column (:=) by applyng the function # from our function above to all of the elements x of the column # As we do it within the data.table, it is really fast! mc_dat[, model := lapply(data, function(x) lm(y ~ x1 + x2 + x3, x)[[&quot;coefficients&quot;]][2])] # now mc_dat[, model[[1]], by=group] gives us our data back as a column # (automatic name V1) with the group. To plot it, we need to extract it as # a vector. Thus, we slice it out just using standard dataframe syntax for ease. hist(mc_dat[,model[[1]], by=group][[&quot;V1&quot;]], breaks=25, col=&quot;red&quot;, main=&quot;Distribution of the estimator for beta_{1}&quot;) 6.2 Optimisation If we choose to program our own estimators, we often compute the value of the estimator by maximising or minimising an objective function. Thus, we need to know how to do optimisation in R. Here, we cover the basic way to do optimisation in R using the optim function. As an example, we show how to program a maximum-likelihood estimator from scratch (as opposed to using say mle). The optim function in base R is a minimiser. It takes the value of a objective function, which we write as a function. It then minimises the function using an algorithm that we specify, given a starting value that we also specify. The first argument is the starting value. The second argument is the function that we want to minimise. The third argument is the method that we want to use to compute it. The default method is the Nelder-Mead algorithm; value BFGS gives the BFGS quasi Newton-Raphson algorithm. Adding hessian=T gives us the Hessian matrix as an output. When we write the function, the vector that we are trying to optimise must be the first argument of the function. We must return the value of the objective function that we are trying to minimise. Thus, if we want to maximise something, we must return the negative of that thing. Lets try it out by programming the maximum likelihood estimator for a linear regression with normally distributed errors using the data from one of the runs of our Monte-Carlo simulation earlier. # Example of optimisation in R - programming the maximum likelihood estimator # for a linear regression with normally distributed errors # Here I borrow from this guide https://www.ime.unicamp.br/~cnaber/optim_1.pdf # we will minimise the negative log-likelihood (equivalent of course to # maximising the normal likelihood function as the location of optima are # preserved under monotonic transformations) # we just do it for one parameter for ease negative_log_likelihood &lt;- function(param_vec, X_mat, y_vec){ beta_vec &lt;- param_vec[1:4] sigma2 &lt;- param_vec[5] n &lt;- nrow(y_vec) mu &lt;- inner_sum &lt;- sum((y_vec-X_mat %*% beta_vec)**2) log_l &lt;- -0.5*n*log(2*pi) -0.5*n*log(sigma2) - (1/(2*sigma2))*sum((y_vec-X_mat %*% beta_vec)**2) return(-log_l) } beta_vec &lt;- optim(c(0,0, 0, 0,1), negative_log_likelihood, method=&quot;BFGS&quot;, hessian = T, X_mat = as.matrix(mc_df[mc_df[[&quot;group&quot;]]==1,c(2:5)]), y_vec = as.matrix(mc_df[mc_df[[&quot;group&quot;]]==1,c(6)])) ## Warning in log(sigma2): NaNs produced beta_vec ## $par ## [1] 0.9999929 0.9724584 -0.2416691 0.3128821 3.5391276 ## ## $value ## [1] 169.0368 ## ## $counts ## function gradient ## 60 11 ## ## $convergence ## [1] 0 ## ## $message ## NULL ## ## $hessian ## [,1] [,2] [,3] [,4] [,5] ## [1,] 2.825555e+09 5.362870e+05 1.293351e+06 3.533071e+06 2.0816105462 ## [2,] 5.362870e+05 1.293351e+02 3.533071e+02 1.054175e+03 0.0006779075 ## [3,] 1.293351e+06 3.533071e+02 1.054175e+03 3.345331e+03 0.0022651463 ## [4,] 3.533071e+06 1.054175e+03 3.345331e+03 1.111938e+04 0.0076538527 ## [5,] 2.081611e+00 6.779075e-04 2.265146e-03 7.653853e-03 -1.7645778030 We see here that this outputs the parameters, the value of the log-likelihood at the minimum, and then some additional statistics. As we selected the Hessian, we can compute the standard errors. # We get the standard errors by taking the square root of the principal diagonal # of the inverse of the Hessian sqrt(diag(solve(beta_vec[[&quot;hessian&quot;]]))) ## Warning in sqrt(diag(solve(beta_vec[[&quot;hessian&quot;]]))): NaNs produced ## [1] 6.556477e-05 1.126158e+00 6.547648e-01 1.124175e-01 NaN 6.3 Profiling Imagine we want to make our code more efficient so it runs quicker. This may sound unnecessary, but it can be important in applied econometric. If we are working with large datasets with hundreds of thousands of rows and doing lots of iteration small inefficiencies can make our code run very slowly as we might be hitting them thousands and thousands of times. The standard way to do this is by profiling our code. The aim is to find the biggest bottleneck we have not dealt with yet, deal with it as best we can, and repeat until our code performs well enough. This might sound trivial. It is not. Thus, programmers typically find the bottlenecks by iteratively testing and timing sections of the code using small, manageable inputs. Here, we take a short look at how to do this in R with the profvis package. For more details, look at the chapter on this in Hadley Wickhams Advanced R (we follow the first section of this chapter here). # loading the library containing our profiler library(profvis) ## Warning: package &#39;profvis&#39; was built under R version 4.0.5 f_1 &lt;- function() { pause(0.5) f_2 } f_2 &lt;- function(){ print(&quot;hello world&quot;) } # we can profile by taking our function, and wrapping it in the function &#39;lineprof()&#39; # we wrap all of the code we want to profile in curly brackets {} profvis({f_1() f_2()}) ## [1] &quot;hello world&quot; The profvis function will open up an interactive profiler where we can look at the relative speed of different parts of our code. This allows us to isolate which bits of the code take more or less time. We can test different versions by wrapping them in the profvis wrapper, and seeing if they take more or less time. # now make the pause slower # we should see that it runs a lot quicker f_3 &lt;- function() { pause(0.25) f_2 } # it does! profvis({f_3() f_2()}) ## [1] &quot;hello world&quot; "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
